{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "# sys, file and nav packages:\n",
    "import datetime as dt\n",
    "import json\n",
    "import functools\n",
    "import time\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "# math packages:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "\n",
    "# charting:\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib import ticker\n",
    "from matplotlib import colors\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import IPython\n",
    "from PIL import Image as PILImage\n",
    "from IPython.display import Markdown as md\n",
    "from IPython.display import display\n",
    "from myst_nb import glue\n",
    "\n",
    "import time\n",
    "\n",
    "unit_label = 'p/100m'\n",
    "\n",
    "# survey data:\n",
    "dfx= pd.read_csv('resources/checked_sdata_eos_2020_21.csv')\n",
    "\n",
    "\n",
    "dfBeaches = pd.read_csv(\"resources/beaches_with_land_use_rates.csv\")\n",
    "dfCodes = pd.read_csv(\"resources/codes_with_group_names_2015.csv\")\n",
    "\n",
    "# set the index of the beach data to location slug\n",
    "dfBeaches.set_index('slug', inplace=True)\n",
    "\n",
    "# set the index of to codes\n",
    "dfCodes.set_index(\"code\", inplace=True)\n",
    "\n",
    "# code description map\n",
    "code_d_map = dfCodes.description.copy()\n",
    "\n",
    "# shorten the descriptions of two codes\n",
    "code_d_map.loc[\"G38\"] = \"sheeting for protecting large cargo items\"\n",
    "code_d_map.loc[\"G73\"] = \"Foamed items & pieces (non packaging/insulation)\"\n",
    "\n",
    "# code material map\n",
    "code_m_map = dfCodes.material\n",
    "\n",
    "# this defines the css rules for the note-book table displays\n",
    "header_row = {'selector': 'th:nth-child(1)', 'props': f'background-color: #FFF; text-align:right'}\n",
    "even_rows = {\"selector\": 'tr:nth-child(even)', 'props': f'background-color: rgba(139, 69, 19, 0.08);'}\n",
    "odd_rows = {'selector': 'tr:nth-child(odd)', 'props': 'background: #FFF;'}\n",
    "table_font = {'selector': 'tr', 'props': 'font-size: 12px;'}\n",
    "table_data = {'selector': 'td', 'props': 'padding: 6px;'}\n",
    "table_css_styles = [even_rows, odd_rows, table_font, header_row]\n",
    "\n",
    "pdtype = pd.core.frame.DataFrame\n",
    "pstype = pd.core.series.Series\n",
    "\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "def scaleTheColumn(x):\n",
    "    \n",
    "    xmin = x.min()\n",
    "    xmax = x.max()\n",
    "    xscaled = (x-xmin)/(xmax-xmin)\n",
    "    \n",
    "    return xscaled\n",
    "\n",
    "def collectAggregateValues(data: pd.DataFrame = None, locations: [] = None, columns: list = [\"location\", \"OBJVAL\"], to_aggregate: str = None):\n",
    "    return data[data.location.isin(locations)].groupby(columns, as_index=False)[to_aggregate].sum()\n",
    "\n",
    "def pivotValues(aggregate_values, index: str = \"location\", columns: str = \"OBJVAL\", values: str = \"surface\"):\n",
    "    return aggregate_values.pivot(index=index, columns=columns, values=values).fillna(0)\n",
    "    \n",
    "def collectAndPivot(data: pd.DataFrame = None, locations: [] = None, columns: list = [\"location\", \"OBJVAL\"], to_aggregate: str = None):\n",
    "    # collects the geo data and aggregates the categories for a 3000 m hex\n",
    "    # the total for each category is the total amount for that category in the specific 3000 m hex\n",
    "    # with the center defined by the survey location.\n",
    "    aggregated = collectAggregateValues(data=data, locations=locations, columns=columns, to_aggregate=to_aggregate)\n",
    "    pivoted = pivotValues(aggregated, index=columns[0], columns=columns[1], values=to_aggregate)\n",
    "    pivoted.columns.name = \"None\"\n",
    "    return pivoted.reset_index(drop=False)\n",
    "\n",
    "def resultsDf(rhovals: pdtype = None, pvals: pdtype = None)-> pdtype:\n",
    "    # masks the values of rho where p > .05\n",
    "    results_df = []\n",
    "    for i, n in enumerate(pvals.index):\n",
    "        arow_of_ps = pvals.iloc[i]\n",
    "        p_fail = arow_of_ps[ arow_of_ps > 0.05]\n",
    "        \n",
    "        arow_of_rhos = rhovals.iloc[i]\n",
    "        \n",
    "        for label in p_fail.index:\n",
    "            if arow_of_rhos[label] == 1:\n",
    "                pass\n",
    "            else:\n",
    "                arow_of_rhos[label] = 0\n",
    "        results_df.append(arow_of_rhos)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def rotateText(x):\n",
    "    return 'writing-mode: vertical-lr; transform: rotate(-180deg);  padding:10px; margins:0; vertical-align: baseline;'\n",
    "\n",
    "def aStyledCorrelationTable(data, columns = ['Obstanlage', 'Reben', 'Siedl', 'Stadtzentr', 'Wald', 'k/n']):\n",
    "    rho = data[columns].corr(method='spearman')\n",
    "    pval = data[columns].corr(method=lambda x, y: stats.spearmanr(x, y)[1])\n",
    "    pless = pd.DataFrame(resultsDf(rho, pval))\n",
    "\n",
    "    pless.columns.name = None\n",
    "    bfr = pless.style.format(precision=2).set_table_styles(table_css_styles)\n",
    "    bfr = bfr.background_gradient(axis=None, vmin=rho.min().min(), vmax=rho.max().max(), cmap=cmap)\n",
    "    bfr = bfr.applymap_index(rotateText, axis=1)\n",
    "    return bfr\n",
    "\n",
    "def addExceededTested(data, exceeded, passed, ratio, tested):\n",
    "    # merges the results of test threshold on to the survey results\n",
    "    data = data.merge(exceeded, left_on='location', right_index=True)\n",
    "    data = data.merge(passed, left_on='location', right_index=True)\n",
    "    data = data.merge(ratio, left_on=\"location\", right_index=True)\n",
    "    data = data.merge(tested, left_on=\"location\", right_index=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def testThreshold(data, threshold, gby_column):\n",
    "    # given a data frame, a threshold and a groupby column\n",
    "    # the given threshold will be tested against the sum\n",
    "    # of aggregated value produced by aggregating on the\n",
    "    # groupby column\n",
    "    \n",
    "    res_code[\"k\"] = data.pcs_m >= threshold\n",
    "    exceeded = data.groupby([gby_column])['k'].sum()\n",
    "    exceeded.name = \"k\"\n",
    "    \n",
    "    tested = data.groupby([gby_column]).loc_date.nunique()\n",
    "    tested.name = 'n'\n",
    "    \n",
    "    passed = tested-exceeded\n",
    "    passed.name = \"n-k\"\n",
    "    \n",
    "    ratio = exceeded/tested\n",
    "    ratio.name = 'k/n'\n",
    "    \n",
    "    return exceeded, tested, passed, ratio\n",
    "\n",
    "class CodeResults:   \n",
    "        \n",
    "    def __init__(self, data, attributes, column=\"pcs_m\", method = stats.spearmanr):        \n",
    "        \n",
    "        self.code_data = data\n",
    "        self.attributes = attributes\n",
    "        self.column = column\n",
    "        self.x = self.code_data[column]\n",
    "        self.method = method\n",
    "       \n",
    "        self.y = None\n",
    "        \n",
    "        super().__init__()\n",
    "    \n",
    "        \n",
    "    def getRho(self, x: np.array = None)-> float:\n",
    "        # assigns y from self        \n",
    "        result = self.method(x, self.y)\n",
    "                               \n",
    "        return result.correlation            \n",
    "        \n",
    "    def exactPValueForRho(self)-> float:\n",
    "        # perform a permutation test instead of relying on \n",
    "        # the asymptotic p-value. Only one of the two inputs \n",
    "        # needs to be shuffled.\n",
    "        p = stats.permutation_test((self.x,) , self.getRho, permutation_type='pairings', n_resamples=1000)\n",
    "        \n",
    "        \n",
    "        return p.pvalue\n",
    "    def rhoForAGroupOfAttributes(self):\n",
    "        # returns the results of the\n",
    "        # the correlation test, including \n",
    "        # a permutation test on p\n",
    "        rhos = []\n",
    "        code = self.code_data.code.unique()[0]\n",
    "        for attribute in self.attributes:\n",
    "            self.y = self.code_data[attribute].values\n",
    "            c= self.getRho(self.x)\n",
    "            p = self.exactPValueForRho()\n",
    "            \n",
    "        # rhos.append({\"code\":code, \"attribute\":attribute, \"c\":c, \"p\":p.statistic})\n",
    "        return {\"code\":code, \"attribute\":attribute, \"c\":c, \"p\":p}\n",
    "\n",
    "\n",
    "def rhoForAGroupOfAttributes(data, attributes, column=\"pcs_m\"):\n",
    "    rhos = []\n",
    "    code = data.code.unique()[0]\n",
    "    for attribute in attributes:\n",
    "        p = CodeResults(data, [attribute],column=column).rhoForAGroupOfAttributes()\n",
    "        rhos.append(p)\n",
    "    return rhos\n",
    "\n",
    "def cleanSurveyResults(data):\n",
    "    # performs data cleaning operations on the\n",
    "    # default data ! this does not remove \n",
    "    # Walensee ! The new map data is complete    \n",
    "    data['loc_date'] = list(zip(data.location, data[\"date\"]))\n",
    "    data['date'] = pd.to_datetime(data[\"date\"])\n",
    "    \n",
    "    # get rid of microplastics\n",
    "    mcr = data[data.groupname == \"micro plastics (< 5mm)\"].code.unique()\n",
    "    \n",
    "    # replace the bad code\n",
    "    data.code = data.code.replace('G207', 'G208')\n",
    "    data = data[~data.code.isin(mcr)]\n",
    "    \n",
    "    # walensee has no landuse values\n",
    "    # data = data[data.water_name_slug != 'walensee']   \n",
    "    \n",
    "    return data\n",
    "\n",
    "class SurveyResults:\n",
    "    \"\"\"Creates a dataframe from a valid filename. Assigns the column names and defines a list of\n",
    "    codes and locations that can be used in the CodeData class.\n",
    "    \"\"\"\n",
    "    \n",
    "    file_name = 'resources/checked_sdata_eos_2020_21.csv'\n",
    "    columns_to_keep=[\n",
    "        'loc_date',\n",
    "        'location', \n",
    "        'river_bassin',\n",
    "        'water_name_slug',\n",
    "        'city',\n",
    "        'w_t', \n",
    "        'intersects', \n",
    "        'code', \n",
    "        'pcs_m',\n",
    "        'quantity'\n",
    "    ]\n",
    "        \n",
    "    def __init__(self, data: str = file_name, clean_data: bool = True, columns: list = columns_to_keep, w_t: str = None):\n",
    "        self.dfx = pd.read_csv(data)\n",
    "        self.df_results = None\n",
    "        self.locations = None\n",
    "        self.valid_codes = None\n",
    "        self.clean_data = clean_data\n",
    "        self.columns = columns\n",
    "        self.w_t = w_t\n",
    "        \n",
    "    def validCodes(self):\n",
    "        # creates a list of unique code values for the data set    \n",
    "        conditions = [\n",
    "            isinstance(self.df_results, pdtype),\n",
    "            \"code\" in self.df_results.columns\n",
    "        ]\n",
    "\n",
    "        if all(conditions):\n",
    "\n",
    "            try:\n",
    "                valid_codes = self.df_results.code.unique()\n",
    "            except ValueError:\n",
    "                print(\"There was an error retrieving the unique code names, self.df.code.unique() failed.\")\n",
    "                raise\n",
    "            else:\n",
    "                self.valid_codes = valid_codes\n",
    "                \n",
    "        \n",
    "    def surveyResults(self):\n",
    "        \n",
    "        # if this method has been called already\n",
    "        # return the result\n",
    "        if self.df_results is not None:\n",
    "            return self.df_results\n",
    "        \n",
    "        # for the default data self.clean data must be called        \n",
    "        if self.clean_data is True:\n",
    "            fd = cleanSurveyResults(self.dfx)\n",
    "            \n",
    "        # if the data is clean then if can be used directly\n",
    "        else:\n",
    "            fd = self.dfx\n",
    "        \n",
    "        # filter the data by the variable w_t\n",
    "        if self.w_t is not None:\n",
    "            fd = fd[fd.w_t == self.w_t]            \n",
    "         \n",
    "        # keep only the required columns\n",
    "        if self.columns:\n",
    "            fd = fd[self.columns]\n",
    "        \n",
    "        # assign the survey results to the class attribute\n",
    "        self.df_results = fd\n",
    "        \n",
    "        # define the list of codes in this df\n",
    "        self.validCodes()\n",
    "        \n",
    "        return self.df_results\n",
    "    \n",
    "    def surveyLocations(self):\n",
    "        if self.locations is not None:\n",
    "            return self.locations\n",
    "        if self.df_results is not None:\n",
    "            self.locations = self.dfResults.location.unique()\n",
    "            return self.locations\n",
    "        else:\n",
    "            print(\"There is no survey data loaded\")\n",
    "            return None\n",
    "\n",
    "def makeACorrelationTable(data, columns, name=None, figsize=(6,9)):\n",
    "    corr = data[columns].corr()\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, ax=ax)\n",
    "\n",
    "    ax.tick_params(axis=\"x\", which=\"both\", labelrotation=90)\n",
    "    ax.tick_params(axis=\"y\", which=\"both\", labelrotation=0)\n",
    "    ax.set_ylabel(\"\")\n",
    "    ax.set_xlabel(\"\")\n",
    "    \n",
    "    glue(name,f, display=False)\n",
    "    plt.close()    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Data -- New map data\n",
    "\n",
    "This document repeats the calculations in the first three sections and incorporates the methods in the fourth section. There are some key differnces:\n",
    "\n",
    "1. There are no excluded regions\n",
    "2. There is geo-data for all regions under consieration\n",
    "3. Only lakes are considered\n",
    "4. The map data is from the most recent release from Swiss Geo Admin\n",
    "\n",
    "## Important changes\n",
    "\n",
    "The land-use categories are labeled differently. The new map data from [swissTLM3d](https://www.swisstopo.admin.ch/fr/geodata/landscape/tlm3d.html#dokumente) does not have the general class agriculture. Any un-labeled surface in the hex was accounted for by subsracting the total of all labeled categories from the surface area of the hex = 5'845'672:\n",
    "\n",
    "```python\n",
    "# there are areas on the map that are not defined by a category.\n",
    "# the total surface area of all categories is subtracted from the\n",
    "# the surface area of a 3000m hex = 5845672\n",
    "area_of_a_hex = 5845672\n",
    "defined_land_cover= land_cover.groupby([ \"river_bass\",\"location\", \"lat\",\"lon\",\"city\",\"feature\"], as_index=False).surface.sum()\n",
    "defined_land_cover[\"OBJVAL\"] = \"undefined\"\n",
    "defined_land_cover[\"surface\"] = area_of_a_hex - defined_land_cover.surface\n",
    "\n",
    "land_cover = pd.concat([land_cover, defined_land_cover])\n",
    "```\n",
    "\n",
    "### Why is this important?\n",
    "\n",
    "1. Repeatability. The old map data was not available.\n",
    "2. Ease of access for researches or stakeholders who wish to verify the results\n",
    "3. Relevance: different map layers could be used but these are the most recent\n",
    "\n",
    "### Why is this better?\n",
    "\n",
    "1. The old map layers used grids of 100 m². The new map layers uses polygons.\n",
    "2. There is no need to transform or manipulate the land-use data once the hex gird overlay is completed\n",
    "\n",
    "__A Hexagon not a circle:__ The buffer is a hexagon of 3'000 m, it is circumscribed by a circle r = 1'500 m. The hexagon allows for better scaling. More of the shoreline conditions can be accounted for when hexagons are used. There is a small loss in the total surface area under consideration. However, hesagonal grids are very easy to implement in QGIS, which means that calculations can be brought to scale quickly.\n",
    "\n",
    "```{figure} resources/images/hex-3k-location.jpeg\n",
    "A 3000 m Hex witht land-use features\n",
    "```\n",
    "\n",
    "The calculations are the same at each Geopraphic level. The number of samples and the mix of landuse attributes changes with every subset of data. It is those changes and how they relate to the magnitude of trash encountered that concerns these enquiries. This document assumes the reader knows what beach-litter monitoring is and how it applies to the health of the environment.\n",
    "\n",
    "A statistical test is not a replacement for common sense. It is an another piece of evidence to consider along with the results from previous studies, the researchers personal experience as well as the history of the problem within the geographic constraints of the data.\n",
    "\n",
    "### Notes\n",
    "\n",
    "__objects of interest__\n",
    "\n",
    "The objects of interest are defined as __those objects where 20 or more were collected at one or more surveys__. In reference to the default data: that is the value of the quantity column.\n",
    "\n",
    "__percent attributed to buildings__\n",
    "\n",
    "There are now multiple possible categories for buildings. The general category _built surface_ is used as well as city center. There is another category that is specific to places of gathering or public use.\n",
    "\n",
    "__method of comparison__\n",
    "\n",
    "The total surface area of each category or subcategory is considered in relation to the survey results. This is a change from the previous sections where the % of total was considered.\n",
    "\n",
    "## The survey data\n",
    "\n",
    "The sampling period for the IQAASL project started in April 2020 and ended May 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# the SurveyResults class will collect the data in the\n",
    "# resources data\n",
    "fdx = SurveyResults()\n",
    "# call the surveyResults method to get the survey data\n",
    "df = fdx.surveyResults()\n",
    "\n",
    "# the lakes of interest\n",
    "collection_points = [\n",
    "    'zurichsee',\n",
    "    'bielersee',\n",
    "    'neuenburgersee',\n",
    "    'walensee',\n",
    "    'vierwaldstattersee',\n",
    "    'brienzersee',\n",
    "    'thunersee',\n",
    "    'lac-leman',\n",
    "    'lago-maggiore',\n",
    "    'lago-di-lugano',\n",
    "    'zugersee'\n",
    "]\n",
    "\n",
    "# the data-frame of survey results to be considered\n",
    "df = df[df.water_name_slug.isin(collection_points)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} resources/images/all_survey_areas_summary.jpeg\n",
    "---\n",
    "name: survey_map\n",
    "---\n",
    "` `\n",
    "```\n",
    "{numref}`figure %s: <survey_map>` All survey locations IQAASL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary data of the surveys, not including locations in the Walensee area:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_128fd tr:nth-child(even) {\n",
       "  background-color: rgba(139, 69, 19, 0.08);\n",
       "}\n",
       "#T_128fd tr:nth-child(odd) {\n",
       "  background: #FFF;\n",
       "}\n",
       "#T_128fd tr {\n",
       "  font-size: 12px;\n",
       "}\n",
       "#T_128fd th:nth-child(1) {\n",
       "  background-color: #FFF;\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_128fd\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_128fd_level0_col0\" class=\"col_heading level0 col0\" >0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_128fd_level0_row0\" class=\"row_heading level0 row0\" >n locations</th>\n",
       "      <td id=\"T_128fd_row0_col0\" class=\"data row0 col0\" >90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_128fd_level0_row1\" class=\"row_heading level0 row1\" >n samples</th>\n",
       "      <td id=\"T_128fd_row1_col0\" class=\"data row1 col0\" >328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_128fd_level0_row2\" class=\"row_heading level0 row2\" >n lake samples</th>\n",
       "      <td id=\"T_128fd_row2_col0\" class=\"data row2 col0\" >328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_128fd_level0_row3\" class=\"row_heading level0 row3\" >n identified object types</th>\n",
       "      <td id=\"T_128fd_row3_col0\" class=\"data row3 col0\" >186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_128fd_level0_row4\" class=\"row_heading level0 row4\" >n possible object types</th>\n",
       "      <td id=\"T_128fd_row4_col0\" class=\"data row4 col0\" >211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_128fd_level0_row5\" class=\"row_heading level0 row5\" >total number of objects</th>\n",
       "      <td id=\"T_128fd_row5_col0\" class=\"data row5 col0\" >48002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fc0ec3cf3a0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The survey results\n",
    "locations = df.location.unique()\n",
    "samples = df.loc_date.unique()\n",
    "lakes = df[df.w_t == \"l\"].drop_duplicates(\"loc_date\").w_t.value_counts().values[0]\n",
    "codes_identified = df[df.quantity > 0].code.unique()\n",
    "codes_possible = df.code.unique()\n",
    "total_id = df.quantity.sum()\n",
    "\n",
    "data_summary = {\n",
    "    \"n locations\": len(locations),\n",
    "    \"n samples\": len(samples),\n",
    "    \"n lake samples\": lakes,\n",
    "    \"n identified object types\": len(codes_identified),\n",
    "    \"n possible object types\": len(codes_possible),\n",
    "    \"total number of objects\": total_id\n",
    "}\n",
    "\n",
    "pd.DataFrame(index = data_summary.keys(), data=data_summary.values()).style.set_table_styles(table_css_styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The map data\n",
    "\n",
    "The proceeding is the land-use categories and the relevant sub-categories. With the exception of _Land Cover_ the total of each category was considered for each hex. The covariance of each sub-category is given in the annex.\n",
    "\n",
    "### The base: Land cover\n",
    "\n",
    "This is how the earth is covered, independent of its use. The following categories are the base land-cover categories:\n",
    "\n",
    "1. Orchard\n",
    "2. Vineyards\n",
    "3. Settlement\n",
    "4. City center\n",
    "5. Forest\n",
    "6. Undefined\n",
    "7. Wetlands\n",
    "\n",
    "The area of each sub-category of land-cover within a 3000 m hex is totaled and the correlation is considered independently. The categories that follow are superimposed on to these surfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# recover the map data \n",
    "mypath=  \"resources/hex-3000m\"\n",
    "map_data = {f.split('.')[0]:pd.read_csv(join(mypath, f)) for f in listdir(mypath) if isfile(join(mypath, f))}\n",
    "\n",
    "# fetch a map\n",
    "land_cover = map_data[\"hex-3000-lake-locations-landcover\"]\n",
    "land_cover.rename(columns={\"undefined\":\"Undefined\"}, inplace=True)\n",
    "\n",
    "# from the data catalog:\n",
    "# https://www.swisstopo.admin.ch/fr/geodata/landscape/tlm3d.html#dokumente\n",
    "land_cover = land_cover[land_cover.location.isin(locations)]\n",
    "\n",
    "# there are areas on the map that are not defined by a category.\n",
    "# the total surface area of all categories is subtracted from the\n",
    "# the surface area of a 3000m hex = 5845672\n",
    "area_of_a_hex = 5845672\n",
    "defined_land_cover= land_cover.groupby([ \"river_bass\",\"location\", \"lat\",\"lon\",\"city\",\"feature\"], as_index=False).surface.sum()\n",
    "defined_land_cover[\"OBJVAL\"] = \"Undefined\"\n",
    "defined_land_cover[\"surface\"] = area_of_a_hex - defined_land_cover.surface\n",
    "\n",
    "land_cover = pd.concat([land_cover, defined_land_cover])\n",
    "\n",
    "\n",
    "# aggregate the geo data for each location\n",
    "# the geo data for the 3000 m hexagon surrounding the survey location\n",
    "# is aggregated into the labled categories, these records get merged with\n",
    "# survey data, keyed on location\n",
    "al_locations = collectAndPivot(data=land_cover, locations=land_cover.location.unique(), columns= [\"location\", \"OBJVAL\"], to_aggregate=\"surface\")\n",
    "\n",
    "# the result is a dataframe with the survey results and the geo data on one row \n",
    "results = df.merge(al_locations, on=\"location\", how='outer', validate=\"many_to_one\")\n",
    "\n",
    "\n",
    "codes = results[results.quantity > 20].code.unique()\n",
    "value_columns = ['Obstanlage', 'Reben', 'Siedl', 'Stadtzentr', 'Wald', 'location', 'Undefined', 'Sumpf', 'water_name_slug', 'code']\n",
    "y_column = [\"pcs_m\"]\n",
    "unique_column = [\"loc_date\"]\n",
    "\n",
    "collect_totals = {}\n",
    "\n",
    "res_codes = [results[results.code == x][[*unique_column, *y_column, *value_columns]] for x in codes]\n",
    "results_dfs = results.drop_duplicates(\"loc_date\")\n",
    "collect_totals.update({\"land_cover\":results_dfs})\n",
    "\n",
    "# test rho for each category and code\n",
    "# an itterative process that takes along time\n",
    "rho_for_land_cover = [pd.DataFrame(rhoForAGroupOfAttributes(x,['Obstanlage', 'Reben', 'Wald','Sumpf', 'Siedl', 'Stadtzentr', 'Undefined'], column=\"pcs_m\")) for x in res_codes]\n",
    "rho_for_land_cover = pd.concat(rho_for_land_cover)\n",
    "\n",
    "# select the pvalues and coefiecients:\n",
    "ps = pd.pivot(rho_for_land_cover, columns=\"attribute\", index=\"code\", values=\"p\")\n",
    "rhos = pd.pivot(rho_for_land_cover, columns=\"attribute\", index=\"code\", values=\"c\")\n",
    "\n",
    "# mask out the results with p > .05\n",
    "landcover = pd.DataFrame(resultsDf(rhos, ps))\n",
    "landcover.columns.name = None\n",
    "landcover.index.name = None\n",
    "\n",
    "# this defines the css rules for the correlation tables\n",
    "table_font = {'selector': 'tr', 'props': 'font-size: 14px;'}\n",
    "table_data = {'selector': 'td', 'props': 'padding: 6px;'}\n",
    "table_correlation_styles = [even_rows, odd_rows, table_font, header_row]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streets\n",
    "\n",
    "```python\n",
    "# from the data catalog:\n",
    "# https://www.swisstopo.admin.ch/fr/geodata/landscape/tlm3d.html#dokumente\n",
    "# the road network has alot of detail, therefore there are\n",
    "# alot of categories. We are interested in the effect of all\n",
    "# methods that could lead to an object being dropped or forgotten\n",
    "# near the location. We take the sum of all road types. \n",
    "# the columns:\n",
    "```\n",
    "\n",
    "The different road sizes that are included.\n",
    "\n",
    "1. 10m road\n",
    "2. 1m path\n",
    "3. 2m path\n",
    "4. 2m path fragment\n",
    "5. 3m road\n",
    "6. 4m road\n",
    "7. 6m road\n",
    "8. 8m road\n",
    "10. exit\n",
    "11. highway\n",
    "12. car road\n",
    "13. sevice road\n",
    "14. entrance\n",
    "15. ferry\n",
    "16. marked lane\n",
    "17. square\n",
    "18. rest area\n",
    "19. link\n",
    "20. access\n",
    "\n",
    "We take the sum of the lengths of all street types within the 3'000 m hex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "hex_data = map_data['hex-3000-lake-locations-strasse']\n",
    "# from the data catalog:\n",
    "# https://www.swisstopo.admin.ch/fr/geodata/landscape/tlm3d.html#dokumente\n",
    "# the road network has alot of detail, therefore there are\n",
    "# alot of categories. We are interested in the effect of all\n",
    "# methods that could lead to an object being dropped or forgotten\n",
    "# near the location. We take the sum of all road types. \n",
    "# the columns:\n",
    "labels = [\n",
    "    '10m Strasse',\n",
    "    '1m Weg',\n",
    "    '1m Wegfragment',\n",
    "    '2m Weg',\n",
    "    '2m Wegfragment',\n",
    "    '3m Strasse',\n",
    "    '4m Strasse',\n",
    "    '6m Strasse',\n",
    "    '8m Strasse',\n",
    "    'Ausfahrt',\n",
    "    'Autobahn',\n",
    "    'Autostrasse',\n",
    "    'Dienstzufahrt',\n",
    "    'Einfahrt',\n",
    "    'Faehre',\n",
    "    'Markierte Spur',\n",
    "    'Platz',\n",
    "    'Raststaette', 'Verbindung', 'Zufahrt'\n",
    "]\n",
    "\n",
    "# the label for the total\n",
    "total_column = \"strasse\"\n",
    "# the geo data and survey data are merged on location\n",
    "merge_column = \"location\"\n",
    "# parameter of interest from the geo data\n",
    "measure = \"length\"\n",
    "# the independent variables of interest\n",
    "value_columns = [total_column, merge_column, 'code']\n",
    "# the value of interest\n",
    "y_column = [\"pcs_m\"]\n",
    "# the unique identifier for each survey\n",
    "unique_column = [\"loc_date\"]\n",
    "# the column labels from the geo data\n",
    "data_columns =  [merge_column, \"OBJEKTART\"]\n",
    "\n",
    "al_hex_data = collectAndPivot(data=hex_data, locations=hex_data.location.unique(), columns=data_columns, to_aggregate=measure)\n",
    "strasse_data = al_hex_data.copy()\n",
    "\n",
    "results = df.merge(al_hex_data, on=merge_column, how='outer', validate=\"many_to_one\")\n",
    "results[total_column] = results[labels].sum(axis=1)\n",
    "res_codes = [results[results.code == x][[*unique_column, *y_column, *value_columns]] for x in codes]\n",
    "results_dfs = results.drop_duplicates(\"loc_date\")\n",
    "collect_totals.update({\"streets\":results_dfs})\n",
    "\n",
    "\n",
    "rho_for_hex_data = [pd.DataFrame(rhoForAGroupOfAttributes(x,[total_column], column=\"pcs_m\")) for x in res_codes]\n",
    "\n",
    "rho_for_hex_data = pd.concat(rho_for_hex_data)\n",
    "ps = pd.pivot(rho_for_hex_data, columns=\"attribute\", index=\"code\", values=\"p\")\n",
    "rhos = pd.pivot(rho_for_hex_data, columns=\"attribute\", index=\"code\", values=\"c\")\n",
    "\n",
    "strasse = pd.DataFrame(resultsDf(rhos, ps))\n",
    "\n",
    "strasse.columns.name = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recreation and sports\n",
    "\n",
    "This is all sporting and recreation areas. Includes camping and swimming pools.\n",
    "\n",
    "```python\n",
    "# from the data catalog:\n",
    "# https://www.swisstopo.admin.ch/fr/geodata/landscape/tlm3d.html#dokumente\n",
    "# Cette Feature Class répertorie des surfaces destinées aux loisirs ou au sport. Des superpositions \n",
    "# entre des polygones avec des types d’objets différents sont autorisées.\n",
    "# these are areas for recreation and sports. They are superimposed over other polygons\n",
    "# like buildings or cities. We are taking the total surface area\n",
    "```\n",
    "\n",
    "1. swimming pool\n",
    "2. campsite\n",
    "3. parade ground, festival area\n",
    "4. golf course\n",
    "4. sports field\n",
    "5. zoo\n",
    "6. recreation area\n",
    "\n",
    "The surface area of these attributes are combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "hex_data = map_data['hex-3000-lake-locations-freizeitareal']\n",
    "# from the data catalog:\n",
    "# https://www.swisstopo.admin.ch/fr/geodata/landscape/tlm3d.html#dokumente\n",
    "# Cette Feature Class répertorie des surfaces destinées aux loisirs ou au sport. Des superpositions \n",
    "# entre des polygones avec des types d’objets différents sont autorisées.\n",
    "# these are areas for recreation and sports. They are superimposed over other polygons\n",
    "# like buildings or cities. We are taking the total surface area\n",
    "labels = hex_data.OBJEKTART.unique()\n",
    "   \n",
    "# the label for the total\n",
    "total_column = \"freizeitareal\"\n",
    "# the geo data and survey data are merged on location\n",
    "merge_column = \"location\"\n",
    "# parameter of interest from the geo data\n",
    "measure = \"surface\"\n",
    "# the independent variables of interest\n",
    "value_columns =  [*labels, \"freizeitareal\",'location', 'code']\n",
    "# the value of interest\n",
    "y_column = [\"pcs_m\"]\n",
    "# the unique identifier for each survey\n",
    "unique_column = [\"loc_date\"]\n",
    "# the column labels from the geo data\n",
    "data_columns =  [merge_column, \"OBJEKTART\"]\n",
    "\n",
    "al_hex_data = collectAndPivot(data=hex_data, locations=hex_data.location.unique(), columns=data_columns, to_aggregate=measure)\n",
    "recreation_data = al_hex_data.copy()\n",
    "\n",
    "results = df.merge(al_hex_data, on=merge_column, how='outer', validate=\"many_to_one\")\n",
    "results[total_column] = results[labels].sum(axis=1)\n",
    "res_codes = [results[results.code == x][[*unique_column, *y_column, *value_columns]] for x in codes]\n",
    "results_dfs = results.drop_duplicates(\"loc_date\")\n",
    "collect_totals.update({\"recreation\":results_dfs})\n",
    "\n",
    "\n",
    "rho_for_hex_data = [pd.DataFrame(rhoForAGroupOfAttributes(x,[total_column], column=\"pcs_m\")) for x in res_codes]\n",
    "\n",
    "rho_for_hex_data = pd.concat(rho_for_hex_data)\n",
    "ps = pd.pivot(rho_for_hex_data, columns=\"attribute\", index=\"code\", values=\"p\")\n",
    "rhos = pd.pivot(rho_for_hex_data, columns=\"attribute\", index=\"code\", values=\"c\")\n",
    "\n",
    "freizeitareal = pd.DataFrame(resultsDf(rhos, ps))\n",
    "freizeitareal.columns.name = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public use\n",
    "\n",
    "These are places like schools, hospitals, official parks, cemeteries\n",
    "\n",
    "```python\n",
    "# from the data catalog:\n",
    "# https://www.swisstopo.admin.ch/fr/geodata/landscape/tlm3d.html#dokumente\n",
    "# Cette Feature Class représente des surfaces utilisées à des fins \n",
    "# spécifiques. Des superpositions entre des polygones avec des types d’objets différents sont autorisées.\n",
    "# this is surface area for specific uses: schools, parks, cemeteries, hospitals. \n",
    "# historical sites. We are taking the total\n",
    "```\n",
    "\n",
    "1. wastewater treatment\n",
    "2. aboriculture\n",
    "3. cemetery\n",
    "4. historic area\n",
    "5. gravel extraction\n",
    "6. monastery\n",
    "7. power plant\n",
    "8. prison\n",
    "9. fair/carnivals\n",
    "10. public park\n",
    "11. allotment garden\n",
    "12. school/university\n",
    "13. hospietal\n",
    "14. quarry\n",
    "15. maintenance facility\n",
    "16. unmanaged forest\n",
    "\n",
    "\n",
    "\n",
    "The surface area of these attributes are combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "hex_data = map_data['hex-3000-lake-locations-nutuzungsareal']\n",
    "hex_data = hex_data[~hex_data[\"OBJEKTART\"].isin([\"Obstanlage\", \"Reben\"])]\n",
    "\n",
    "# from the data catalog:\n",
    "# https://www.swisstopo.admin.ch/fr/geodata/landscape/tlm3d.html#dokumente\n",
    "# Cette Feature Class représente des surfaces utilisées à des fins \n",
    "# spécifiques. Des superpositions entre des polygones avec des types d’objets différents sont autorisées.\n",
    "# this is surface area for specific uses: schools, parks, cemeteries, hospitals. \n",
    "# historical sites. We are taking the total\n",
    "labels = [\n",
    "    'Abwasserreinigungsareal',\n",
    "    'Baumschule',\n",
    "    'Friedhof',\n",
    "    'Historisches Areal',\n",
    "    'Kiesabbauareal',\n",
    "    'Klosterareal',\n",
    "    'Kraftwerkareal',\n",
    "    'Massnahmenvollzugsanstaltsareal',\n",
    "    'Messeareal',\n",
    "    'Oeffentliches Parkareal',\n",
    "    'Schrebergartenareal',\n",
    "    'Schul- und Hochschulareal',\n",
    "    'Spitalareal',\n",
    "    'Steinbruchareal',\n",
    "    'Unterwerkareal',\n",
    "    'Wald nicht bestockt',\n",
    "]\n",
    "   \n",
    "# the label for the total\n",
    "total_column = \"nutuzungsareal\"\n",
    "# the geo data and survey data are merged on location\n",
    "merge_column = \"location\"\n",
    "# parameter of interest from the geo data\n",
    "measure = \"surface\"\n",
    "# the independent variables of interest\n",
    "value_columns =  [*labels, \"nutuzungsareal\",'location', 'code']\n",
    "# the value of interest\n",
    "y_column = [\"pcs_m\"]\n",
    "# the unique identifier for each survey\n",
    "unique_column = [\"loc_date\"]\n",
    "# the column labels from the geo data\n",
    "data_columns =  [merge_column, \"OBJEKTART\"]\n",
    "\n",
    "al_hex_data = collectAndPivot(data=hex_data, locations=hex_data.location.unique(), columns=data_columns, to_aggregate=measure)\n",
    "public_use = al_hex_data.copy()\n",
    "\n",
    "results = df.merge(al_hex_data, on=merge_column, how='outer', validate=\"many_to_one\")\n",
    "results[total_column] = results[labels].sum(axis=1)\n",
    "res_codes = [results[results.code == x][[*unique_column, *y_column, *value_columns]] for x in codes]\n",
    "results_dfs = results.drop_duplicates(\"loc_date\")\n",
    "collect_totals.update({\"public_use\":results_dfs})\n",
    "\n",
    "\n",
    "rho_for_hex_data = [pd.DataFrame(rhoForAGroupOfAttributes(x,[total_column], column=\"pcs_m\")) for x in res_codes]\n",
    "\n",
    "rho_for_hex_data = pd.concat(rho_for_hex_data)\n",
    "ps = pd.pivot(rho_for_hex_data, columns=\"attribute\", index=\"code\", values=\"p\")\n",
    "rhos = pd.pivot(rho_for_hex_data, columns=\"attribute\", index=\"code\", values=\"c\")\n",
    "\n",
    "nutuzungsareal = pd.DataFrame(resultsDf(rhos, ps))\n",
    "nutuzungsareal.columns.name = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of river network and distance to intersection\n",
    "\n",
    "See section [Condisder distance to river](distancetoriver) for the explanation on how these values are derived from the Geo data.\n",
    "\n",
    "```python\n",
    "# from the data catalog:\n",
    "# https://www.swisstopo.admin.ch/fr/geodata/landscape/tlm3d.html#dokumente\n",
    "# Cette Feature Class répertorie les cours d’eau sous forme de lignes. \n",
    "# Les lignes sont orientées dans le sens du courant.\n",
    "# the intersect data is derived. the labels are from the rivers layer and the\n",
    "# the survey points layer. \n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "# permutation is not required here.\n",
    "# there are > 900 records, each result is keyed\n",
    "# to each intersection in the buffer. Therefore\n",
    "# there are 983 records for each code.\n",
    "\n",
    "def collectCorrelation(data, codes, columns):\n",
    "    results = []\n",
    "    for code in codes:\n",
    "        d = data[data.code == code]\n",
    "        dx = d.pcs_m.values\n",
    "        for name in columns:\n",
    "            dy = d[name].values\n",
    "            c, p = stats.spearmanr(dx, dy)\n",
    "            \n",
    "            results.append({\"code\":code, \"variable\":name, \"rho\":c, \"p\":p})\n",
    "    return results\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# the intersect data\n",
    "dtoi_o = pd.read_csv(\"resources/buffer_output/distance_to_intersection.csv\")\n",
    "\n",
    "# from the data catalog:\n",
    "# https://www.swisstopo.admin.ch/fr/geodata/landscape/tlm3d.html#dokumente\n",
    "# Cette Feature Class répertorie les cours d’eau sous forme de lignes. \n",
    "# Les lignes sont orientées dans le sens du courant.\n",
    "# the intersect data is derived. the labels are from the rivers layer and the\n",
    "# the survey points layer. \n",
    "\n",
    "# the distance and lenght attributes are on separate layers\n",
    "# rename the columns and create a column to merge on with the lenght attribute\n",
    "columns = [ \"river_bass\", \"feature\", \"city\", \"location\", \"NAMN_2\", \"BREITE\", \"KLASSE_2\", \"HOC\", \"feature\", \"distance\", \"OBJVAL\"]\n",
    "dtoi = dtoi_o[columns].copy()\n",
    "rename = {\"NAMN_2\":\"name\", \"BREITE\":\"size\", \"KLASSE_2\":\"class\", \"HOC\":\"hoc\", \"NAMN\":\"name\",\"KLASSE\":\"class\"}\n",
    "dtoi.rename(columns=rename, inplace=True)\n",
    "\n",
    "# designate a column to merge on\n",
    "dtoi[\"merge_col\"] = list(zip(dtoi.location, dtoi[\"name\"], dtoi[\"size\"], dtoi[\"class\"]))\n",
    "dtoi.drop_duplicates(\"merge_col\", inplace=True)\n",
    "\n",
    "# the length data\n",
    "dtoi_l = pd.read_csv(\"resources/buffer_output/intersection_length.csv\")\n",
    "columns = [\"river_bass\", \"feature\", \"city\", \"location\", \"NAMN\", \"BREITE\", \"KLASSE\", \"HOC\", \"feature\", \"length\", \"OBJVAL\"]\n",
    "dtol = dtoi_l[columns].copy()\n",
    "dtol.rename(columns=rename, inplace=True)\n",
    "\n",
    "# designate a column to merge on\n",
    "dtol[\"merge_col\"] = list(zip(dtol.location, dtol[\"name\"], dtol[\"size\"], dtol[\"class\"]))\n",
    "\n",
    "# merge the length and intersection data\n",
    "these_merge_cols = [\"length\",\"name\",\"merge_col\"]\n",
    "ind = dtoi.merge(dtol[these_merge_cols], on=\"merge_col\")\n",
    "ind = ind[[\"location\", \"name_x\",\"distance\", \"length\", \"size\", \"class\"]].copy()\n",
    "\n",
    "# merge the attributes to the survey data\n",
    "ints_and_data = df[[\"loc_date\",\"location\", \"city\", \"code\", \"pcs_m\"]].merge(ind, on=\"location\")\n",
    "\n",
    "# only the codes and locations of interest\n",
    "locations = df.location.unique()\n",
    "data = ints_and_data[(ints_and_data.code.isin(codes)) & (ints_and_data.location.isin(locations))].copy()\n",
    "data.fillna(0, inplace=True)\n",
    "distance_data = data.drop_duplicates([\"loc_date\", \"distance\"])\n",
    "collect_totals.update({\"distance\":distance_data[[\"loc_date\", \"distance\"]]})\n",
    "collect_totals.update({\"length\":distance_data[[\"loc_date\", \"length\"]]})\n",
    "# the size and class of river come with the map layer\n",
    "# the class is from the ocean, for example the Rhone and the Aare have\n",
    "# the same class.\n",
    "columns = [\"distance\", \"length\", \"size\", \"class\"]\n",
    "\n",
    "# permutation is not required here.\n",
    "# there are > 900 records, each result is keyed\n",
    "# to each intersection in the buffer. Therefore\n",
    "# there are 983 records for each code.\n",
    "def collectCorrelation(data, codes, columns):\n",
    "    results = []\n",
    "    for code in codes:\n",
    "        d = data[data.code == code]\n",
    "        dx = d.pcs_m.values\n",
    "        for name in columns:\n",
    "            dy = d[name].values\n",
    "            c, p = stats.spearmanr(dx, dy)\n",
    "            \n",
    "            results.append({\"code\":code, \"variable\":name, \"rho\":c, \"p\":p})\n",
    "    return results\n",
    "\n",
    "corellation_results = collectCorrelation(data, codes, columns)\n",
    "crp = pd.DataFrame(corellation_results)\n",
    "pvals = crp.pivot(index=\"code\", columns=\"variable\", values=\"p\")\n",
    "rhovals = crp.pivot(index=\"code\", columns=\"variable\", values=\"rho\")\n",
    "d_and_l = pd.DataFrame(resultsDf(rhovals, pvals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### The number of river intersections\n",
    "\n",
    "The total number of river intersections within 1 500 m radius is considered. The number of intersections is from the previous map layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# the number of intersects is included with the survey results\n",
    "# it is also included with the beach data\n",
    "value_columns = ['intersects','location', 'code']\n",
    "\n",
    "y_column = [\"pcs_m\"]\n",
    "unique_column = [\"loc_date\"]\n",
    "total_column = \"intersects\"\n",
    "\n",
    "res_codes = [df[df.code == x][[*unique_column, *y_column, *value_columns]] for x in codes]\n",
    "df_intersects = df.drop_duplicates(\"loc_date\")\n",
    "collect_totals.update({\"intersects\":df_intersects[[\"loc_date\", \"intersects\"]]})\n",
    "\n",
    "rho_for_ints = [pd.DataFrame(rhoForAGroupOfAttributes(x,[total_column], column=\"pcs_m\")) for x in res_codes]\n",
    "\n",
    "rho_for_ints = pd.concat(rho_for_ints)\n",
    "ps = pd.pivot(rho_for_ints, columns=\"attribute\", index=\"code\", values=\"p\")\n",
    "rhos = pd.pivot(rho_for_ints, columns=\"attribute\", index=\"code\", values=\"c\")\n",
    "\n",
    "intersects = pd.DataFrame(resultsDf(rhos, ps))\n",
    "\n",
    "intersects.columns.name = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The land-use profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# merge the derived land use values into a data frame\n",
    "lc = collect_totals[\"land_cover\"][['loc_date','Obstanlage', 'Reben', 'Wald','Sumpf', 'Siedl', 'Stadtzentr', 'Undefined']]\n",
    "lc= lc.merge(collect_totals['public_use'][['loc_date', 'nutuzungsareal']], on=\"loc_date\")\n",
    "lc = lc.merge(collect_totals['recreation'][['loc_date', 'freizeitareal']], on=\"loc_date\")\n",
    "\n",
    "# format the m² and linear meters to km² and Km\n",
    "lc[lc.columns[1:]] = lc[lc.columns[1:]] * 0.000001\n",
    "lc = lc.merge(collect_totals['streets'][[\"loc_date\", \"strasse\"]], on=\"loc_date\")\n",
    "lc[[\"strasse\"]] = lc[[\"strasse\"]]/1000  \n",
    "\n",
    "# merge the intersects\n",
    "lc = lc.merge(collect_totals['intersects'][[\"loc_date\", \"intersects\"]], on=\"loc_date\")\n",
    "\n",
    "# the distance and river network are handled separately because \n",
    "# there is more than one value for most locations ie. most locations have more than one intersec\n",
    "ic = collect_totals['distance'][['loc_date', 'distance']]\n",
    "ic = ic.merge(collect_totals['length'][['loc_date', 'length']], on=\"loc_date\")\n",
    "ic[[\"length\", \"distance\"]] = ic[[\"length\", \"distance\"]]/1000\n",
    "ic = ic.rename(columns={'distance':'Distance to intersect [km]', 'length':'Length of river network [km]'})\n",
    "\n",
    "column_names = {\n",
    "    \"Obstanlage\":\"Orchard [km²]\",\n",
    "    \"Reben\":\"Vineyard [km²]\",\n",
    "    \"Wald\":\"Woods [km²]\",\n",
    "    \"Sumpf\":\"Wetlands [km²]\",\n",
    "    \"intersects\":\"Intersects [#]\",\n",
    "    \"Stadtzentr\":\"City-center [km²]\",\n",
    "    \"Siedl\":\"Buildings/Urban [km²]\",    \n",
    "    \"strasse\":\"Roads/streets [km]\",\n",
    "    \"freizeitareal\":\"Recreation/sports [km²]\",\n",
    "    \"nutuzungsareal\":\"Schools/infrastructure [km²]\",    \n",
    "    \"Undefined\":\"Undefined [km²]\",\n",
    "}\n",
    "\n",
    "lc = lc.rename(columns=column_names)\n",
    "\n",
    "# method to get the ranked correlation of pcs_m to each explanatory variable\n",
    "def make_plot_with_spearmans(data, ax, n):\n",
    "    sns.scatterplot(data=data, x=n, y=unit_label, ax=ax, color='black', s=30, edgecolor='white', alpha=0.6)\n",
    "    corr, a_p = stats.spearmanr(data[n], data[unit_label])\n",
    "    return ax, corr, a_p\n",
    "\n",
    "fig, axs = plt.subplots(4,4, figsize=(10,10), sharey=False)\n",
    "\n",
    "\n",
    "\n",
    "data = lc\n",
    "for i,n in enumerate(lc.columns[1:]):\n",
    "    column = i%4\n",
    "    row=int(i/4)\n",
    "    ax = axs[row, column]\n",
    "    \n",
    "    # the ECDF of the land use variable\n",
    "    the_data = ECDF(data[n].values)\n",
    "    sns.lineplot(x=the_data.x, y= (the_data.y), ax=ax, color='dodgerblue', label=\"cumulative distribution\" )\n",
    "    \n",
    "    # get the median % of land use for each variable under consideration from the data\n",
    "    the_median = data[n].median()\n",
    "    \n",
    "    # plot the median and drop horzontal and vertical lines\n",
    "    ax.scatter([the_median], .5, color='red',s=50, linewidth=2, zorder=100, label=\"the median\")\n",
    "    ax.vlines(x=the_median, ymin=0, ymax=.5, color='red', linewidth=2)\n",
    "    ax.hlines(xmax=the_median, xmin=0, y=0.5, color='red', linewidth=2)\n",
    "    \n",
    "    #remove the legend from ax   \n",
    "    ax.get_legend().remove()\n",
    "    \n",
    "    if column == 0:\n",
    "        ax.set_ylabel(\"Share of \\nsurveys [%]\", labelpad = 15)\n",
    "    else:\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    # add the median value from all locations to the ax title\n",
    "    ax.set_title(f'{n}\\nmedian={round(the_median, 2)}',fontsize=10, loc='left')\n",
    "    ax.set_xlabel(\" \")\n",
    "    ax.set_ylim(0,1)\n",
    "    \n",
    "data = ic    \n",
    "for j, k in enumerate(ic.columns[1:]):\n",
    "    column = (i+1+j)%4\n",
    "    row=int((i+1+j)/4)\n",
    "    ax = axs[row, column]    \n",
    "    \n",
    "    # the ECDF of the land use variable\n",
    "    the_data = ECDF(data[k].values)\n",
    "    sns.lineplot(x=the_data.x, y= (the_data.y), ax=ax, color='dodgerblue', label=\"cumulative distribution\" )\n",
    "    \n",
    "    # get the median % of land use for each variable under consideration from the data\n",
    "    the_median = data[k].median()\n",
    "    \n",
    "    # plot the median and drop horzontal and vertical lines\n",
    "    ax.scatter([the_median], .5, color='red',s=50, linewidth=2, zorder=100, label=\"the median\")\n",
    "    ax.vlines(x=the_median, ymin=0, ymax=.5, color='red', linewidth=2)\n",
    "    ax.hlines(xmax=the_median, xmin=0, y=0.5, color='red', linewidth=2)\n",
    "    \n",
    "    #remove the legend from ax   \n",
    "    ax.get_legend().remove()\n",
    "    \n",
    "    if column == 0:\n",
    "        ax.set_ylabel(\"Share of \\nsurveys [%]\", labelpad = 15)\n",
    "    else:\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    # add the median value from all locations to the ax title\n",
    "    ax.set_title(f'{k}\\nmedian={round(the_median, 2)}',fontsize=10, loc='left')\n",
    "    ax.set_xlabel(\" \")\n",
    "    ax.set_ylim(0,1)\n",
    "    \n",
    "for anax in [axs[3,1], axs[3,2], axs[3,3]]:\n",
    "    anax.grid(False)\n",
    "    anax.set_xticks([])\n",
    "    anax.set_yticks([])\n",
    "    anax.spines[\"top\"].set_visible(False)\n",
    "    anax.spines[\"right\"].set_visible(False)\n",
    "    anax.spines[\"bottom\"].set_visible(False)\n",
    "    anax.spines[\"left\"].set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "h, l = ax.get_legend_handles_labels()\n",
    "\n",
    "axs[3,2].legend(h,l, loc=\"center\", bbox_to_anchor=(.5,.5))\n",
    "plt.tight_layout()\n",
    "\n",
    "glue(\"the_land_use_profile\", fig, display=False)\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{glue:figure} the_land_use_profile\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rho with the new map data\n",
    "\n",
    "The total columns from the precedding sections are merged with the land cover section. In this version we account for more objects while homogenizing the sampling conditions. The removal of rivers is tantamount to removing zeroes given the consistently low survey results in the class. The exception is Lavey-les-Bains, situated jsut down stream from a damn and collected when the river was particularly low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "all_results = pd.concat([landcover, strasse.strasse, freizeitareal.freizeitareal, nutuzungsareal.nutuzungsareal, d_and_l.distance, d_and_l.length, intersects.intersects],axis=1).fillna(0)\n",
    "\n",
    "column_names = {\n",
    "    \"Obstanlage\":\"Orchard\",\n",
    "    \"Reben\":\"Vineyard\",    \n",
    "    \"Wald\":\"Woods\",\n",
    "    \"Sumpf\":\"Wetlands\",\n",
    "    \"Undefined\":\"Undefined\",\n",
    "    \"Siedl\":\"Buildings/Urban\",\n",
    "    \"Stadtzentr\":\"City-center\",\n",
    "    \"strasse\":\"Meters of roads/streets\",\n",
    "    \"freizeitareal\":\"Recreation/sports\",\n",
    "    \"nutuzungsareal\":\"Schools/amusement-parks/infrastructure\",\n",
    "    \"distance\": \"Distance to river intersection\",\n",
    "    \"length\": \"Length of river network\",\n",
    "    \"intersects\": \"Intersects\"\n",
    "    \n",
    "}\n",
    "\n",
    "col_order = [\n",
    "    \"Obstanlage\",\n",
    "    \"Reben\",\n",
    "    \"Wald\",\n",
    "    \"Sumpf\",\n",
    "    \"Undefined\",\n",
    "    \"Siedl\",\n",
    "    \"Stadtzentr\",\n",
    "    \"strasse\",\n",
    "    \"freizeitareal\",\n",
    "    \"nutuzungsareal\",\n",
    "    \"distance\",\n",
    "    \"length\",\n",
    "    \"intersects\"\n",
    "]\n",
    "\n",
    "all_results[\"desc\"] = all_results.index.map(lambda x: code_d_map.loc[x])\n",
    "all_results.set_index(\"desc\", drop=True, inplace=True)\n",
    "all_results = all_results[col_order]\n",
    "\n",
    "all_results.rename(columns=column_names, inplace=True)\n",
    "all_results.columns.name = None\n",
    "all_results.index.name = None\n",
    "bfr = all_results.style.format(precision=2).set_table_styles(table_css_styles)\n",
    "bfr = bfr.background_gradient(axis=None, vmin=all_results.min().min(), vmax=all_results.max().max(), cmap=cmap)\n",
    "bfr = bfr.applymap_index(rotateText, axis=1)\n",
    "glue('rho_new_map_data', bfr, display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{glue:figure} rho_new_map_data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance of land-use categories\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "makeACorrelationTable(all_results, all_results.columns, name=\"correlation_land_use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{glue:figure} correlation_land_use\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total correlations, total positive correlations, total negative corrrelations for each buffer radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def countTheNumberOfCorrelations(data):\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for acol in data.columns:\n",
    "        d = data[acol]\n",
    "        pos = (d > 0).sum()\n",
    "        neg = (d < 0).sum()\n",
    "        \n",
    "        results.append({\"attribute\":acol, \"positive\":pos, \"negative\":neg, \"total\": pos + neg})\n",
    "    return results\n",
    "\n",
    "total_correlations = pd.DataFrame(countTheNumberOfCorrelations(all_results))\n",
    "total_correlations.set_index(\"attribute\", drop=True, inplace=True)\n",
    "total_correlations.index.name = None\n",
    "bfr = total_correlations.style.format(precision=2).set_table_styles(table_css_styles)\n",
    "glue('total_correlations', bfr, display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{glue:figure} total_correlations\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annex\n",
    "\n",
    "### Print version of rho values\n",
    "\n",
    "resources/output/tlm-3d-2022-land-use.jpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "all_results = all_results.round(2)\n",
    "fig, ax = plt.subplots(figsize=(16,12))\n",
    "\n",
    "sns.heatmap(all_results,cmap=cmap, yticklabels=True, linecolor=\"white\", linewidths=.1, ax=ax, cbar=True, center=0,square=True)\n",
    "plt.tight_layout()\n",
    "\n",
    "glue('the_print_version', fig, display=False)\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{glue:figure} the_print_version\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance of subcategories of each land use category\n",
    "\n",
    "#### Land Cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "al_locations.rename(columns={\"Reben\":\"vineyeards\", \"Siedl\":\"town/city\", \"Obstanlage\":\"Orchard\", \"Stadtzentr\":\"City center\", \"Wald\":\"Forest\", \"Sumpf\":\"Wetland\"}, inplace=True)    \n",
    "makeACorrelationTable(al_locations, al_locations.columns[1:], name=\"land_cover\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{glue:figure} land_cover\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Streets roads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "makeACorrelationTable(strasse_data, strasse_data.columns[1:], name=\"streets_roads\", figsize=(10,11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{glue:figure} streets_roads\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recreation and sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "makeACorrelationTable(recreation_data, recreation_data.columns[1:], name=\"recreation_sports\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{glue:figure} recreation_sports\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Public or specific use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "makeACorrelationTable(public_use, public_use.columns[1:], name=\"public_specific\", figsize=(10,11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{glue:figure} public_specific\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance and length to intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "makeACorrelationTable(distance_data, distance_data.columns[-4:], figsize=(4,4), name=\"distance_length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{glue:figure} distance_length\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "This script updated 16/04/2023 in Biel, CH\n",
       "\n",
       "> ❤️ what you do everyday\n",
       "\n",
       "*analyst at hammerdirt*\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today = dt.datetime.now().date().strftime(\"%d/%m/%Y\")\n",
    "where = \"Biel, CH\"\n",
    "\n",
    "my_block = f\"\"\"\n",
    "\n",
    "This script updated {today} in {where}\n",
    "\n",
    "> \\u2764\\ufe0f what you do everyday\n",
    "\n",
    "*analyst at hammerdirt*\n",
    "\"\"\"\n",
    "\n",
    "md(my_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Git repo: https://github.com/hammerdirt-analyst/landuse.git\n",
      "\n",
      "Git branch: main\n",
      "\n",
      "numpy     : 1.24.2\n",
      "json      : 2.0.9\n",
      "PIL       : 9.5.0\n",
      "pandas    : 2.0.0\n",
      "scipy     : 1.10.1\n",
      "matplotlib: 3.7.1\n",
      "IPython   : 8.12.0\n",
      "seaborn   : 0.12.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark --iversions -b -r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}