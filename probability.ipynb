{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "# sys, file and nav packages:\n",
    "import datetime as dt\n",
    "import json\n",
    "import functools\n",
    "import time\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import typing\n",
    "\n",
    "# math packages:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "\n",
    "# charting:\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib import ticker\n",
    "from matplotlib import colors\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import IPython\n",
    "from PIL import Image as PILImage\n",
    "from IPython.display import Markdown as md\n",
    "from IPython.display import display\n",
    "from myst_nb import glue\n",
    "\n",
    "import time\n",
    "\n",
    "unit_label = 'p/100m'\n",
    "\n",
    "# survey data:\n",
    "dfx= pd.read_csv('resources/checked_sdata_eos_2020_21.csv')\n",
    "\n",
    "\n",
    "dfBeaches = pd.read_csv(\"resources/beaches_with_land_use_rates.csv\")\n",
    "dfCodes = pd.read_csv(\"resources/codes_with_group_names_2015.csv\")\n",
    "\n",
    "# set the index of the beach data to location slug\n",
    "dfBeaches.set_index('slug', inplace=True)\n",
    "\n",
    "# set the index of to codes\n",
    "dfCodes.set_index(\"code\", inplace=True)\n",
    "\n",
    "# code description map\n",
    "code_d_map = dfCodes.description.copy()\n",
    "\n",
    "# shorten the descriptions of two codes\n",
    "code_d_map.loc[\"G38\"] = \"sheeting for protecting large cargo items\"\n",
    "code_d_map.loc[\"G73\"] = \"Foamed items & pieces (non packaging/insulation)\"\n",
    "\n",
    "# code material map\n",
    "code_m_map = dfCodes.material\n",
    "\n",
    "# this defines the css rules for the note-book table displays\n",
    "header_row = {'selector': 'th:nth-child(1)', 'props': f'background-color: #FFF; text-align:right'}\n",
    "even_rows = {\"selector\": 'tr:nth-child(even)', 'props': f'background-color: rgba(139, 69, 19, 0.08);'}\n",
    "odd_rows = {'selector': 'tr:nth-child(odd)', 'props': 'background: #FFF;'}\n",
    "table_font = {'selector': 'tr', 'props': 'font-size: 12px;'}\n",
    "table_data = {'selector': 'td', 'props': 'padding: 6px;'}\n",
    "table_css_styles = [even_rows, odd_rows, table_font, header_row]\n",
    "\n",
    "pdtype = pd.core.frame.DataFrame\n",
    "pstype = pd.core.series.Series\n",
    "\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "def scaleTheColumn(x):\n",
    "    \n",
    "    xmin = x.min()\n",
    "    xmax = x.max()\n",
    "    xscaled = (x-xmin)/(xmax-xmin)\n",
    "    \n",
    "    return xscaled\n",
    "\n",
    "def cleanSurveyResults(data):\n",
    "    # performs data cleaning operations on the\n",
    "    # default data ! this does not remove \n",
    "    # Walensee ! The new map data is complete    \n",
    "    data['loc_date'] = list(zip(data.location, data[\"date\"]))\n",
    "    data['date'] = pd.to_datetime(data[\"date\"])\n",
    "    \n",
    "    # get rid of microplastics\n",
    "    mcr = data[data.groupname == \"micro plastics (< 5mm)\"].code.unique()\n",
    "    \n",
    "    # replace the bad code\n",
    "    data.code = data.code.replace('G207', 'G208')\n",
    "    data = data[~data.code.isin(mcr)]\n",
    "    \n",
    "    return data\n",
    "\n",
    "class SurveyResults:\n",
    "    \"\"\"Creates a dataframe from a valid filename. Assigns the column names and defines a list of\n",
    "    codes and locations that can be used in the CodeData class.\n",
    "    \"\"\"\n",
    "    \n",
    "    file_name = 'resources/checked_sdata_eos_2020_21.csv'\n",
    "    columns_to_keep=[\n",
    "        'loc_date',\n",
    "        'location', \n",
    "        'river_bassin',\n",
    "        'water_name_slug',\n",
    "        'city',\n",
    "        'w_t', \n",
    "        'intersects', \n",
    "        'code', \n",
    "        'pcs_m',\n",
    "        'quantity'\n",
    "    ]\n",
    "        \n",
    "    def __init__(self, data: str = file_name, clean_data: bool = True, columns: list = columns_to_keep, w_t: str = None):\n",
    "        self.dfx = pd.read_csv(data)\n",
    "        self.df_results = None\n",
    "        self.locations = None\n",
    "        self.valid_codes = None\n",
    "        self.clean_data = clean_data\n",
    "        self.columns = columns\n",
    "        self.w_t = w_t\n",
    "        \n",
    "    def validCodes(self):\n",
    "        # creates a list of unique code values for the data set    \n",
    "        conditions = [\n",
    "            isinstance(self.df_results, pdtype),\n",
    "            \"code\" in self.df_results.columns\n",
    "        ]\n",
    "\n",
    "        if all(conditions):\n",
    "\n",
    "            try:\n",
    "                valid_codes = self.df_results.code.unique()\n",
    "            except ValueError:\n",
    "                print(\"There was an error retrieving the unique code names, self.df.code.unique() failed.\")\n",
    "                raise\n",
    "            else:\n",
    "                self.valid_codes = valid_codes\n",
    "                \n",
    "        \n",
    "    def surveyResults(self):\n",
    "        \n",
    "        # if this method has been called already\n",
    "        # return the result\n",
    "        if self.df_results is not None:\n",
    "            return self.df_results\n",
    "        \n",
    "        # for the default data self.clean data must be called        \n",
    "        if self.clean_data is True:\n",
    "            fd = cleanSurveyResults(self.dfx)\n",
    "            \n",
    "        # if the data is clean then if can be used directly\n",
    "        else:\n",
    "            fd = self.dfx\n",
    "        \n",
    "        # filter the data by the variable w_t\n",
    "        if self.w_t is not None:\n",
    "            fd = fd[fd.w_t == self.w_t]            \n",
    "         \n",
    "        # keep only the required columns\n",
    "        if self.columns:\n",
    "            fd = fd[self.columns]\n",
    "        \n",
    "        # assign the survey results to the class attribute\n",
    "        self.df_results = fd\n",
    "        \n",
    "        # define the list of codes in this df\n",
    "        self.validCodes()\n",
    "        \n",
    "        return self.df_results\n",
    "    \n",
    "    def surveyLocations(self):\n",
    "        if self.locations is not None:\n",
    "            return self.locations\n",
    "        if self.df_results is not None:\n",
    "            self.locations = self.dfResults.location.unique()\n",
    "            return self.locations\n",
    "        else:\n",
    "            print(\"There is no survey data loaded\")\n",
    "            return None\n",
    "        \n",
    "\n",
    "def surface_area_of_feature(data, locations, columns):\n",
    "    d = data[data.location.isin(locations)]\n",
    "    d = d.groupby(columns, as_index=False).surface.sum()\n",
    "    \n",
    "    return d\n",
    "def account_for_undefined(data, var_col=\"OBJVAL\", var_label=\"Undefined\", metric=\"surface\", total_metric=None):\n",
    "    data[var_col] = var_label\n",
    "    data[metric] = total_metric - data[metric]\n",
    "    \n",
    "    return data\n",
    "\n",
    "def check_file_type(files: list = None):\n",
    "    return files\n",
    "\n",
    "def collect_feature_data(path_to_data: str = \"resources/hex-3000m\"):\n",
    "    # checks the files in the path_to_data are .csv\n",
    "    # applies pd.DataFrame to each .csv and stores results\n",
    "    # in a dictionary where key = name of map and value is\n",
    "    # the corresponding dataframe\n",
    "    \n",
    "    files = listdir(path_to_data)\n",
    "    \n",
    "    files = check_file_type(files)\n",
    "    \n",
    "    data_map = {f.split('.')[0]:pd.read_csv(join(path_to_data, f)) for f in files}\n",
    "    \n",
    "    return data_map\n",
    "\n",
    "def collectAggregateValues(data: pd.DataFrame = None, locations: [] = None, columns: list = [\"location\", \"OBJVAL\"],\n",
    "                           to_aggregate: str = None):\n",
    "    return data[data.location.isin(locations)].groupby(columns, as_index=False)[to_aggregate].sum()\n",
    "\n",
    "def pivotValues(aggregate_values, index: str = \"location\", columns: str = \"OBJVAL\", values: str = \"surface\"):\n",
    "    return aggregate_values.pivot(index=index, columns=columns, values=values).fillna(0)\n",
    "    \n",
    "def collectAndPivot(data: pd.DataFrame = None, locations: [] = None, columns: list = [\"location\", \"OBJVAL\"],\n",
    "                    to_aggregate: str = None, scale_data: bool= False ):\n",
    "    # collects the geo data and aggregates the categories for a 3000 m hex\n",
    "    # the total for each category is the total amount for that category in the specific 3000 m hex\n",
    "    # with the center defined by the survey location.\n",
    "    aggregated = collectAggregateValues(data=data, locations=locations, columns=columns, to_aggregate=to_aggregate)\n",
    "    pivoted = pivotValues(aggregated, index=columns[0], columns=columns[1], values=to_aggregate)\n",
    "    pivoted.columns.name = \"None\"\n",
    "    \n",
    "    if scale_data is True:\n",
    "        pivoted[pivoted.columns[1:]] = pivoted[pivoted.columns[1:]].apply(lambda x: scale_the_column(x))\n",
    "        \n",
    "    return pivoted.reset_index(drop=False)\n",
    "\n",
    "def define_the_objects_of_interest(data, param: str = None, param_value: typing.Union[float, int] = 0, param_label: str = None):\n",
    "    # returns a list of objects whose survey value was\n",
    "    # greater than the threshold value\n",
    "    \n",
    "    return data[data[param] > param_value][param_label].unique()\n",
    "\n",
    "def make_merge_kwargs(on: str = \"location\", how: str = \"outer\", validate: str = \"many_to_one\"):\n",
    "    return dict(on=on, how=how, validate=validate)\n",
    "\n",
    "def merge_test_results_and_land_use_attributes(test_results, land_use_values, **kwargs):\n",
    "   \n",
    "    results = test_results.merge(land_use_values, **kwargs)\n",
    "    return results\n",
    "\n",
    "def test_threshhold(data, threshold, gby_column):\n",
    "    # given a data frame, a threshold and a groupby column\n",
    "    # the given threshold will be tested against the aggregated\n",
    "    # value produced by the groupby column    \n",
    "    data[\"k\"] = data.pcs_m > threshold\n",
    "    exceeded = data.groupby([gby_column])['k'].sum()\n",
    "    exceeded.name = \"k\"\n",
    "    \n",
    "    tested = data.groupby([gby_column]).loc_date.nunique()\n",
    "    tested.name = 'n'\n",
    "    \n",
    "    passed = tested-exceeded\n",
    "    passed.name = \"n-k\"\n",
    "    \n",
    "    ratio = exceeded/tested\n",
    "    ratio.name = 'k/n'\n",
    "    \n",
    "    return exceeded, tested, passed, ratio\n",
    "\n",
    "def test_one_object(data, threshold, gby_column):\n",
    "    exceeded, tested, passed, ratio = test_threshhold(data, threshold, gby_column)\n",
    "    tested = pd.concat([exceeded, tested, passed, ratio], axis=1)    \n",
    "    \n",
    "    return tested\n",
    "\n",
    "def group_land_use_values(collected_and_pivoted, columns, quantile, labels):\n",
    "    \"\"\"For groups that are considered cover and not use the magnitude of the polygon\n",
    "    is taken into consideration. \n",
    "    \n",
    "    The resulting magnitudes are grouped according to the quantile variable\n",
    "    \"\"\"\n",
    "    \n",
    "    for column in columns:\n",
    "        collected_and_pivoted[column] = pd.qcut(collected_and_pivoted[column], q=quantile, duplicates='drop')\n",
    "    \n",
    "    return collected_and_pivoted\n",
    "\n",
    "def land_use_is_present(collected_and_pivoted, columns):\n",
    "    \"\"\"For groups that are superimposed or occurr infrequently only the presence\n",
    "    is noted and the success rate.\n",
    "    \n",
    "    An example is a school, it takes up very little space but it does generate alot of\n",
    "    activity.\n",
    "    \"\"\"\n",
    "    \n",
    "    for column in columns:\n",
    "        if column in collected_and_pivoted.columns:\n",
    "            collected_and_pivoted[column] = (collected_and_pivoted[column] > 0)*1\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    return collected_and_pivoted\n",
    "\n",
    "def inference_for_one_attribute(data, attribute, operation):\n",
    "    \"\"\"The data are grouped according to attribute and aggregated\n",
    "    according to operation. The results are tallied here\n",
    "    \n",
    "    The index is on the attribute range and labeled according\n",
    "    to the index position. It can be accessed in two ways\n",
    "    and searched by magnitude.\n",
    "    \"\"\"\n",
    "    print(attribute)\n",
    "    \n",
    "    d = data.groupby(attribute).agg(operation)\n",
    "    \n",
    "    d[\"n-k\"] = d[\"n\"]-d[\"k\"]\n",
    "    d[\"rate\"] = d[\"k\"]/d[\"n\"]\n",
    "    d[\"odds\"] = d[\"k\"]/d[\"n-k\"]\n",
    "    d[\"pass_rate\"] = d[\"n-k\"]/d[\"n\"]\n",
    "    total_probability = d[\"rate\"].sum()\n",
    "    d[\"posterior\"] = d[\"rate\"]/total_probability\n",
    "    d.reset_index(inplace=True, drop=False)\n",
    "    d[\"label\"] = d.index\n",
    "    d.set_index(attribute, drop=True, inplace=True)\n",
    "    \n",
    "    return d\n",
    "\n",
    "class LandUseValues:\n",
    "    \n",
    "    def __init__(self, data_map: pd.DataFrame = None, locations: list = None, region: str = None, columns: list = None,\n",
    "                 id_vals: list = None, dim_oi: int = None, to_aggregate: str = None, land_use_groups: list = None):\n",
    "        \n",
    "        self.data_map = data_map\n",
    "        self.locations = locations\n",
    "        self.region = region\n",
    "        self.columns = columns\n",
    "        self.dim_oi = dim_oi\n",
    "        self.to_aggregate = to_aggregate\n",
    "        self.id_vals = id_vals\n",
    "        self.land_use_groups = land_use_groups\n",
    "        self.land_cover = None\n",
    "        self.length_of = None\n",
    "        self.land_use = None\n",
    "        \n",
    "        \n",
    "        \n",
    "        # return super()__init__(self)\n",
    "    \n",
    "    def assign_undefined(self):\n",
    "        \n",
    "        # from the data catalog:\n",
    "        # https://www.swisstopo.admin.ch/fr/geodata/landscape/tlm3d.html#dokumente\n",
    "        defined_land_cover = surface_area_of_feature(self.data_map, self.locations, self.columns)\n",
    "\n",
    "        # there are areas on the map that are not defined by a category.\n",
    "        # the total surface area of all categories is subtracted from the\n",
    "        # the surface area of a 3000m hex = 5845672\n",
    "        defined_land_cover = account_for_undefined(defined_land_cover, total_metric=self.dim_oi)\n",
    "        \n",
    "        # add the undefined land-use values to the to the defined ones \n",
    "        land_cover = pd.concat([self.data_map, defined_land_cover])\n",
    "        \n",
    "        # aggregate the geo data for each location\n",
    "        # the geo data for the 3000 m hexagon surrounding the survey location\n",
    "        # is aggregated into the labled categories, these records get merged with\n",
    "        # survey data, keyed on location\n",
    "        kwargs = dict(data=land_cover, locations=self.locations, columns=self.id_vals, to_aggregate=self.to_aggregate)\n",
    "        al_locations = collectAndPivot(**kwargs)\n",
    "        \n",
    "        self.land_cover = al_locations\n",
    "        \n",
    "    def define_total_length(self, label: str = \"total\"):\n",
    "        kwargs = dict(data=self.data_map, locations=self.locations, columns=self.id_vals, to_aggregate=self.to_aggregate)\n",
    "        al_locations = collectAndPivot(**kwargs)\n",
    "        al_locations[label] = al_locations[self.land_use_groups].sum(axis=1)        \n",
    "        \n",
    "        self.length_of = al_locations[[\"location\", label]]\n",
    "        \n",
    "    def define_total_surface(self, label: str = \"total\"):\n",
    "        kwargs = dict(data=self.data_map, locations=self.locations, columns=self.id_vals, to_aggregate=self.to_aggregate)\n",
    "        al_locations = collectAndPivot(**kwargs)\n",
    "        al_locations[label] = al_locations[self.land_use_groups].sum(axis=1)\n",
    "        \n",
    "        self.land_use = al_locations[[\"location\", label]]   \n",
    "        \n",
    "        \n",
    "        \n",
    "class TestResultsAndLandUse(LandUseValues):\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame = None, threshhold: typing.Union[float, int] = None, merge_column: str = None,\n",
    "                 merge_method: str = None, merge_validate: str = None, groups: list = None, presence: list = None,\n",
    "                 quantiles: list = None, labels: list = None, **kwargs):\n",
    "        \n",
    "        self.to_test = df\n",
    "        self.threshhold = threshhold\n",
    "        self.merge_column = merge_column\n",
    "        self.merge_method = merge_method\n",
    "        self.merge_validate = merge_validate\n",
    "        self.groups = groups\n",
    "        self.presence = presence\n",
    "        self.quantiles = quantiles\n",
    "        self.labels = labels\n",
    "        \n",
    "        return super().__init__(**kwargs)\n",
    "    \n",
    "    \n",
    "    def test_and_merge(self, cover: bool = True):\n",
    "        \n",
    "        tested = test_one_object(self.to_test, self.threshhold, self.merge_column)\n",
    "       \n",
    "        kwargs = dict(on=self.merge_column, how=self.merge_method, validate=self.merge_validate)\n",
    "        \n",
    "        if cover is True:\n",
    "            results = merge_test_results_and_land_use_attributes(tested, self.land_cover, **kwargs)\n",
    "        else:\n",
    "            results = merge_test_results_and_land_use_attributes(tested, self.length_of, **kwargs)\n",
    "            \n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def make_groups_test_presence(self, cover: bool = True, label: str = \"total\", regional_label: str = None, label_map: pd.Series = None):\n",
    "        \n",
    "        results = self.test_and_merge(cover=cover)\n",
    "        # print(results.head())\n",
    "        \n",
    "        if self.groups is not None:\n",
    "            if not cover:\n",
    "                self.groups = [label]\n",
    "            results = group_land_use_values(results, self.groups, self.quantiles, self.labels)\n",
    "            \n",
    "        if self.presence is not None:\n",
    "            results = land_use_is_present(results, self.presence)\n",
    "            \n",
    "        if regional_label is not None:\n",
    "            results[regional_label] = results[merge_column].apply(lambda x: label_map.loc[x])\n",
    "            \n",
    "        \n",
    "        \n",
    "        return results\n",
    "    \n",
    "    \n",
    "\n",
    "class InferenceGroups:\n",
    "    \n",
    "    def __init__(self, results: pd.DataFrame = None, column_names: list = None, operation: dict = None):\n",
    "        \n",
    "        self.results = results\n",
    "        self.column_names = column_names\n",
    "        self.operation = operation\n",
    "        self.inf_groups = None\n",
    "        \n",
    "        return super().__init__()\n",
    "    \n",
    "    \n",
    "    def make_inference_for_each_attribute(self):\n",
    "        \n",
    "        res = {}\n",
    "        \n",
    "        for name in self.column_names:\n",
    "            if name in self.results.columns:\n",
    "                inf = inference_for_one_attribute(self.results, name, self.operation)\n",
    "                res.update({name:inf})\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        self.inf_groups = res\n",
    "        \n",
    "    def apply_infgroup_labels_to_results(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "def attach_inference_group_labels_to_survey_data(fg: pd.DataFrame = None, inf_groups: dict = None, groups_and_presence: list = None):\n",
    "    \n",
    "    newfg = fg.copy()\n",
    "    \n",
    "    # order the columns according to groups_and_presence\n",
    "    ls = [x for x in newfg.columns if x not in groups_and_presence]\n",
    "    xgt = newfg[ls]\n",
    "    cols = [x for x in groups_and_presence if x in newfg.columns]\n",
    "    xgl = newfg[[\"location\", *cols]]\n",
    "    newfg = xgt.merge(xgl, on=\"location\")\n",
    "\n",
    "    \n",
    "    for label in cols:     \n",
    "        \n",
    "        \n",
    "        ifg = inf_groups.inf_groups[label]\n",
    "       \n",
    "        newfg[label] = newfg[label].apply(lambda x: ifg.loc[x, \"label\"])\n",
    "        \n",
    "        \n",
    "    return newfg, cols\n",
    "\n",
    "def tries_or_fails(df, columns, probability_tables, product=True, tries_or_fails=\"k\"):\n",
    "    data=df.copy()\n",
    "    for x in columns:\n",
    "        w = probability_tables[x]\n",
    "        data[x] = data[x].apply(lambda x: w.loc[x, tries_or_fails])\n",
    "    \n",
    "    if product:\n",
    "        data[\"total\"] = data[columns].prod(axis=1)\n",
    "    else:\n",
    "        data[\"total\"] = data[columns].sum(axis=1)\n",
    "        \n",
    "    \n",
    "    return data\n",
    "\n",
    "class LanduseConfiguration:\n",
    "    \n",
    "    def __init__(self, land_use_kwargs: dict = None, test_kwargs: dict = None, label: str = None, assign_undefined: bool = False,\n",
    "                 length: bool = False, cover: bool = False, inf_operation: dict = {\"k\":\"sum\", \"n\":\"sum\"}, regional_label: str = None, \n",
    "                 label_map: pd.Series = None, total: bool = False):\n",
    "        \n",
    "        self.land_use_kwargs = land_use_kwargs\n",
    "        self.test_kwargs = test_kwargs\n",
    "        self.label = label\n",
    "        self.label_map = label_map\n",
    "        self.regional_label = regional_label\n",
    "        self.assign_undefined = assign_undefined\n",
    "        self.length = length\n",
    "        self.cover = cover\n",
    "        self.inf_operation = inf_operation\n",
    "        self.total = total\n",
    "        self.test_results = None\n",
    "        self.grouped_data = None\n",
    "        self.inf_groups = None\n",
    "        self.p_tables = None\n",
    "        \n",
    "        \n",
    "        return super().__init__()\n",
    "    \n",
    "    def groups_and_presence(self):\n",
    "        \n",
    "        d = TestResultsAndLandUse(**self.test_kwargs, **self.land_use_kwargs)\n",
    "        \n",
    "        if self.length is True:\n",
    "            d.define_total_length(label=self.label)\n",
    "        if self.cover is True:\n",
    "            d.assign_undefined()\n",
    "        if self.regional_label is not None:\n",
    "            dg = d.make_groups_test_presence(regional_label=self.regional_label, label_map=self.label_map)\n",
    "        else:\n",
    "            dg = d.make_groups_test_presence(cover=self.cover, label=self.label)\n",
    "            \n",
    "        self.test_results = d\n",
    "        self.grouped_data = dg\n",
    "        \n",
    "    def inference_groups(self):        \n",
    "        # create the inference groups for this land use class\n",
    "        # define the column names of the inference groups\n",
    "        if self.total is True:\n",
    "            column_names = [self.label]\n",
    "        else:\n",
    "            column_names = [*test_kwargs[\"groups\"], *test_kwargs[\"presence\"]]\n",
    "            \n",
    "        if self.grouped_data is None:\n",
    "            self.groups_and_presence()\n",
    "        \n",
    "        # group the land use features by magnitude\n",
    "        inf_groups = InferenceGroups(results=self.grouped_data, column_names=column_names, operation=self.inf_operation)\n",
    "        inf_groups.make_inference_for_each_attribute()\n",
    "        \n",
    "        # attach the the inference group labels to the survey data for each feature\n",
    "        labeled_groups, cols = attach_inference_group_labels_to_survey_data(self.grouped_data, inf_groups, column_names)\n",
    "        labeled_groups[\"conf\"] = list(zip(*[labeled_groups[x] for x in cols]))\n",
    "        configuration_keys = labeled_groups[[\"location\", \"conf\"]].set_index(\"location\")\n",
    "        \n",
    "        self.inf_groups = inf_groups\n",
    "        self.p_tables = inf_groups.inf_groups\n",
    "        self.labeled_groups = labeled_groups\n",
    "        self.configuration_keys = configuration_keys\n",
    "\n",
    "def select_a_land_use_conf(conf, p_tables: dict = None, vals_to_drop: tuple=None):\n",
    "    \"\"\"Takes the land use confiuguration from a location and sums the number of trials\n",
    "    and successes. \n",
    "    \n",
    "    Vals to drop gives the option to eliminate all matching land use categories that appear\n",
    "    in the conf. Example if a locations has conf (0,1,2,3,4)  and another location has conf\n",
    "    (2,3,4,5,6) then land-use categories 2, 3, 4 are only counted once.\n",
    "    \n",
    "    returns a tuple k=success and n=trials\n",
    "    \"\"\"\n",
    "    \n",
    "    k = 0\n",
    "    n = 0\n",
    "    for i, pair in enumerate(conf):\n",
    "        if vals_to_drop is not None and vals_to_drop[i][1] == pair[1]:\n",
    "            pass\n",
    "        else:\n",
    "            # print(pair)\n",
    "            # print(p_tables[pair[0]])\n",
    "            d = p_tables[pair[0]]\n",
    "            \n",
    "            e = d.loc[d.label == pair[1],[\"k\",\"n\"] ].values[0]\n",
    "            \n",
    "            k += e[0]\n",
    "            n += e[1]    \n",
    "    \n",
    "    return k, n\n",
    "\n",
    "def select_a_group_of_confs(conf, p_tables, drop_vals: tuple=None):    \n",
    "    \"\"\"Takes an array of location configurations (confs) and applies\n",
    "    select_a_land_use_conf to each one.\n",
    "    \n",
    "    returns a tuple k=success and n = trials\n",
    "    \"\"\"\n",
    "    \n",
    "    failed = 0\n",
    "    tried = 0   \n",
    "    for i, pair in enumerate(conf):\n",
    "        d = p_tables[pair[0]]\n",
    "        attribute = pair[0]\n",
    "        d_index = d.index.name\n",
    "        \n",
    "        if attribute != d_index:\n",
    "            print(attribute, d_index)\n",
    "            print(\"ouch\")\n",
    "        # collect = [x for x in d.label if x != pair[1]]\n",
    "        \n",
    "        e = d[[\"k\", \"n\"]].sum()\n",
    "        failed += e[0]\n",
    "        tried += e[1]\n",
    "    return failed, tried\n",
    "\n",
    "def inferenceTableForOneLocation(name: str = None, lake: str = None, conf: tuple = None, p_tables: dict = None,\n",
    "                                 tested_groups_presence: pd.DataFrame = None, tested: pd.DataFrame = None,\n",
    "                                 prior: float = None, conf_names: list = None, drop_vals: tuple = None):\n",
    "    \n",
    "    # h1 the threshold was exceeded at the location given the land use values\n",
    "    # likelihood of exceeding the threshold given the land use in that hex\n",
    "    lk, ln = select_a_land_use_conf(conf=conf, p_tables=p_tables, vals_to_drop=drop_vals)\n",
    "    failed = lk/ln\n",
    "    # h2 the threshold was not exceeded with that land use configuration\n",
    "    passed = (ln-lk)/ln\n",
    "    \n",
    "    # total probability\n",
    "    # the threshold was exceeded under any land use configuration\n",
    "    tk = [v[\"k\"].sum() for k, v in p_tables.items()]\n",
    "    tk = sum(tk)\n",
    "    \n",
    "    # the number of tries\n",
    "    tn = [v[\"n\"].sum() for k, v in p_tables.items()]\n",
    "    tn = sum(tn)\n",
    "    \n",
    "    # the threshold was not exceeded\n",
    "    passed_t = (tn-tk)/tn\n",
    "    failed_t = tk/tn\n",
    "\n",
    "    if prior is not None:\n",
    "        p = prior\n",
    "    else:\n",
    "        # assign a prior from the survey results\n",
    "        # if there is data for the location in question\n",
    "        # use it.        \n",
    "        pkn, pnn = tested.loc[tested.location == name, ['k', 'n']].sum().values\n",
    "                \n",
    "        if pkn == 0:\n",
    "            p = (pkn+1)/(pnn+2)\n",
    "        elif pkn/pnn == 1:\n",
    "            p = (pkn+1)/(pnn+2)\n",
    "        else:\n",
    "            p = (pkn+1)/(pnn+2)\n",
    "    \n",
    "    m = ((1-p)*passed_t) +(p*failed_t)\n",
    "    \n",
    "    # print(f'conf failed: {round(failed, 3)}, conf passed: {round(passed, 3)}, total failed: {round(failed_t, 3)}, total passed: {round(passed_t, 3)}, prior: {round(p, 3)}')\n",
    "    \n",
    "    return (failed*p)/m\n",
    "\n",
    "def inference_for_one_location(location: str = None, lake: str = None, conf_label: str='configuration',\n",
    "                              conf_columns: list = None, p_tables: dict = None, prior: float = None,\n",
    "                              tested: pd.DataFrame = None, drop_vals: tuple = None):\n",
    "    conf = fgl_conf_keys.loc[location, \"configuration\"]\n",
    "    conf = tuple(zip(conf_columns, conf))\n",
    "\n",
    "    p = inferenceTableForOneLocation(\n",
    "        name =location,\n",
    "        lake=lake,\n",
    "        conf_names = conf_columns,\n",
    "        conf=conf,\n",
    "        p_tables=p_tables,\n",
    "        tested=tested,\n",
    "        prior=prior,\n",
    "        drop_vals=drop_vals\n",
    "    )\n",
    "    \n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_dfc71 tr:nth-child(even) {\n",
       "  background-color: rgba(139, 69, 19, 0.08);\n",
       "}\n",
       "#T_dfc71 tr:nth-child(odd) {\n",
       "  background: #FFF;\n",
       "}\n",
       "#T_dfc71 tr {\n",
       "  font-size: 12px;\n",
       "}\n",
       "#T_dfc71 th:nth-child(1) {\n",
       "  background-color: #FFF;\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_dfc71\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_dfc71_level0_col0\" class=\"col_heading level0 col0\" >0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_dfc71_level0_row0\" class=\"row_heading level0 row0\" >n locations</th>\n",
       "      <td id=\"T_dfc71_row0_col0\" class=\"data row0 col0\" >93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dfc71_level0_row1\" class=\"row_heading level0 row1\" >n samples</th>\n",
       "      <td id=\"T_dfc71_row1_col0\" class=\"data row1 col0\" >331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dfc71_level0_row2\" class=\"row_heading level0 row2\" >n lake samples</th>\n",
       "      <td id=\"T_dfc71_row2_col0\" class=\"data row2 col0\" >331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dfc71_level0_row3\" class=\"row_heading level0 row3\" >n identified object types</th>\n",
       "      <td id=\"T_dfc71_row3_col0\" class=\"data row3 col0\" >186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dfc71_level0_row4\" class=\"row_heading level0 row4\" >n possible object types</th>\n",
       "      <td id=\"T_dfc71_row4_col0\" class=\"data row4 col0\" >211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dfc71_level0_row5\" class=\"row_heading level0 row5\" >total number of objects</th>\n",
       "      <td id=\"T_dfc71_row5_col0\" class=\"data row5 col0\" >48060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f812409e730>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the SurveyResults class will collect the data in the\n",
    "# resources data folder\n",
    "fdx = SurveyResults()\n",
    "# call the surveyResults method to get the survey data\n",
    "df = fdx.surveyResults()\n",
    "\n",
    "# the name of the lake was changed in the revision\n",
    "# change it to the correct name\n",
    "df_none = df[df.water_name_slug != 'quatre-cantons']\n",
    "df_with = df[df.water_name_slug == 'quatre-cantons'].copy()\n",
    "df_with['water_name_slug'] = 'vierwaldstattersee'\n",
    "df = pd.concat([df_none, df_with])\n",
    "\n",
    "# the lakes of interest\n",
    "# collection points would be a good\n",
    "# label. This is how they are considered in the model\n",
    "# these locations serve as the endpoint for many \n",
    "# small rivers\n",
    "collection_points = [\n",
    "    'zurichsee',\n",
    "    'bielersee',\n",
    "    'neuenburgersee',\n",
    "    'walensee',\n",
    "    'vierwaldstattersee',\n",
    "    'brienzersee',\n",
    "    'thunersee',\n",
    "    'lac-leman',\n",
    "    'lago-maggiore',\n",
    "    'lago-di-lugano',\n",
    "    'zugersee'\n",
    "]\n",
    "\n",
    "# the data-frame of survey results to be considered\n",
    "df = df[df.water_name_slug.isin(collection_points)]\n",
    "\n",
    "# map the location to the name of the lake\n",
    "wn_map = df[[\"location\", \"water_name_slug\"]].drop_duplicates(\"location\").set_index(\"location\")\n",
    "wn_map = wn_map[\"water_name_slug\"]\n",
    "\n",
    "\n",
    "# The summary of the survey data\n",
    "locations = df.location.unique()\n",
    "samples = df.loc_date.unique()\n",
    "lakes = df[df.w_t == \"l\"].drop_duplicates(\"loc_date\").w_t.value_counts().values[0]\n",
    "codes_identified = df[df.quantity > 0].code.unique()\n",
    "codes_possible = df.code.unique()\n",
    "total_id = df.quantity.sum()\n",
    "\n",
    "data_summary = {\n",
    "    \"n locations\": len(locations),\n",
    "    \"n samples\": len(samples),\n",
    "    \"n lake samples\": lakes,\n",
    "    \"n identified object types\": len(codes_identified),\n",
    "    \"n possible object types\": len(codes_possible),\n",
    "    \"total number of objects\": total_id\n",
    "}\n",
    "\n",
    "pd.DataFrame(index = data_summary.keys(), data=data_summary.values()).style.set_table_styles(table_css_styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary data of the surveys, not including locations in the Walensee area:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The map data\n",
    "\n",
    "The proceeding is the land-use categories and the relevant sub-categories. With the exception of _Land Cover_ the total of each category was considered for each hex. The covariance of each sub-category is given in the annex.\n",
    "\n",
    "### The base: Land cover\n",
    "\n",
    "This is how the earth is covered, independent of its use. The following categories are the base land-cover categories:\n",
    "\n",
    "1. Orchard\n",
    "2. Vineyards\n",
    "3. Settlement\n",
    "4. City center\n",
    "5. Forest\n",
    "6. Undefined\n",
    "7. Wetlands\n",
    "\n",
    "The area of each sub-category of land-cover within a 3000 m hex is totaled and the correlation is considered independently. The categories that follow are superimposed on to these surfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "my_path=  \"resources/hex-3000m\"\n",
    "\n",
    "columns = [\n",
    "    \"river_bass\",\n",
    "    \"location\", \n",
    "    \"lat\",\"lon\",\n",
    "    \"city\",\n",
    "    \"feature\"\n",
    "]\n",
    "\n",
    "column_rename = {\"undefined\":\"Undefined\"}\n",
    "area_of_a_hex = 5845672\n",
    "id_vals = [\"location\", \"OBJVAL\"]\n",
    "agg_val = \"surface\"\n",
    "\n",
    "data_map_name = \"hex-3000-lake-locations-landcover\"\n",
    "\n",
    "lc_groups = [\n",
    "    'Siedl',\n",
    "    'Undefined',\n",
    "    'Wald'\n",
    "]\n",
    "\n",
    "lc_presence = [\n",
    "    'Obstanlage',\n",
    "    'Reben',\n",
    "    'Stadtzentr',\n",
    "    'Sumpf',\n",
    "    'Fels'\n",
    "]\n",
    "\n",
    "groups_and_presence = [*lc_groups, *lc_presence]\n",
    "       \n",
    "merge_column = gby_column = \"location\"\n",
    "merge_method = 'outer'\n",
    "merge_validate = \"many_to_one\"\n",
    "quantiles =   [0,.1, .25, .5, .75, .9, 1]\n",
    "\n",
    "        \n",
    "# test the survey results of an object of interest\n",
    "# against a threshhold\n",
    "code = \"Gfoam\"\n",
    "to_test = df[df.code == code].copy()\n",
    "threshhold = to_test.pcs_m.median()\n",
    "\n",
    "# label the region of interest\n",
    "# this creates a hierarcal group of the locations\n",
    "# that are within a region. The most common would be lake\n",
    "regional_label = \"lake\"\n",
    "label_map = wn_map\n",
    "\n",
    "\n",
    "# collect the data\n",
    "data_map = collect_feature_data(path_to_data=my_path)\n",
    "land_cover_data = data_map[data_map_name]\n",
    "land_cover_data.rename(columns=column_rename, inplace=True)\n",
    "\n",
    "# limit the data to the parameters of interest\n",
    "land_cover_data = land_cover_data[land_cover_data[id_vals[1]].isin(groups_and_presence)].copy()\n",
    "\n",
    "land_use_kwargs = {\n",
    "    \"data_map\":land_cover_data,\n",
    "    \"locations\":locations,\n",
    "    \"region\":None,\n",
    "    \"columns\":columns,\n",
    "    \"id_vals\": id_vals,\n",
    "    \"dim_oi\": area_of_a_hex,\n",
    "    \"to_aggregate\":agg_val,\n",
    "    \"land_use_groups\":lc_groups,\n",
    "}\n",
    "\n",
    "test_kwargs = {\n",
    "    \"df\":to_test,\n",
    "    \"threshhold\":threshhold,\n",
    "    \"merge_column\":merge_column,\n",
    "    \"merge_method\":merge_method,\n",
    "    \"merge_validate\":merge_validate,\n",
    "    \"groups\":lc_groups,\n",
    "    \"presence\":lc_presence,\n",
    "    \"quantiles\":quantiles\n",
    "}\n",
    "\n",
    "inf_operation = {\"k\":\"sum\", \"n\":\"sum\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Siedl\n",
      "Undefined\n",
      "Wald\n",
      "Obstanlage\n",
      "Reben\n",
      "Stadtzentr\n",
      "Sumpf\n",
      "Fels\n"
     ]
    }
   ],
   "source": [
    "kwargs = {\n",
    "    'land_use_kwargs': land_use_kwargs,\n",
    "    'test_kwargs': test_kwargs,\n",
    "    'label': None,\n",
    "    'assign_undefined': True,\n",
    "    'length': False,\n",
    "    'cover': True,\n",
    "    'inf_operation': inf_operation,\n",
    "    'regional_label': regional_label,\n",
    "    'label_map': label_map,\n",
    "    'total': False\n",
    "}\n",
    "\n",
    "\n",
    "nx = LanduseConfiguration(**kwargs)\n",
    "nx.groups_and_presence()\n",
    "nx.inference_groups()\n",
    "nxf = nx.grouped_data\n",
    "nx_conf = nx.labeled_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strasse\n"
     ]
    }
   ],
   "source": [
    "# add the data from another land use layer\n",
    "# the results will be added to the landcover data frame\n",
    "label = \"strasse\"\n",
    "data_map_name = 'hex-3000-lake-locations-strasse'\n",
    "id_vals = [\"location\", \"OBJEKTART\"]\n",
    "agg_val = \"length\"\n",
    "\n",
    "# identify the columns of interest\n",
    "st_groups = [\n",
    "    '10m Strasse',\n",
    "    '1m Weg',\n",
    "    '1m Wegfragment',\n",
    "    '2m Weg',\n",
    "    '2m Wegfragment',\n",
    "    '3m Strasse',\n",
    "    '4m Strasse',\n",
    "    '6m Strasse',\n",
    "    '8m Strasse',\n",
    "    'Ausfahrt',\n",
    "    'Autobahn',\n",
    "    'Autostrasse',\n",
    "    'Dienstzufahrt',\n",
    "    'Einfahrt',\n",
    "    'Faehre',\n",
    "    'Markierte Spur',\n",
    "    'Platz',\n",
    "    'Raststaette', 'Verbindung', 'Zufahrt'\n",
    "]\n",
    "\n",
    "st_presence = []\n",
    "\n",
    "groups_and_presence = [*st_groups, *st_presence]\n",
    "\n",
    "# collect the data\n",
    "street_lengths_data = data_map[data_map_name]\n",
    "street_lengths_data.rename(columns=column_rename, inplace=True)\n",
    "\n",
    "# update the key word arguments for the land use data\n",
    "land_use_update = dict(data_map=street_lengths_data, id_vals=id_vals, to_aggregate=agg_val, land_use_groups=st_groups)\n",
    "land_use_kwargs.update(land_use_update)\n",
    "\n",
    "# update the key word arguments for the test data\n",
    "test_update = dict(groups=st_groups, presence=None)\n",
    "test_kwargs.update(test_update)\n",
    "\n",
    "kwargs = {\n",
    "    'land_use_kwargs': land_use_kwargs,\n",
    "    'test_kwargs': test_kwargs,\n",
    "    'label': 'strasse',\n",
    "    'assign_undefined': False,\n",
    "    'length': True,\n",
    "    'cover': False,\n",
    "    'inf_operation': inf_operation,\n",
    "    'regional_label': None,\n",
    "    'label_map': None,\n",
    "    'total': True\n",
    "}\n",
    "\n",
    "strasse = LanduseConfiguration(**kwargs)\n",
    "\n",
    "strasse.groups_and_presence()\n",
    "strasse.inference_groups()\n",
    "sxf = strasse.grouped_data\n",
    "sx_conf = strasse.labeled_groups\n",
    "strasse_config = strasse.configuration_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recreation\n"
     ]
    }
   ],
   "source": [
    "# add the data from another land use layer\n",
    "# the results will be added to the landcover data frame\n",
    "column_rename = {\"undefined\":\"Undefined\"}\n",
    "data_map_name = 'hex-3000-lake-locations-freizeitareal'\n",
    "id_vals = [\"location\", \"OBJEKTART\"]\n",
    "agg_val = \"surface\"\n",
    "\n",
    "recreation_data = data_map[data_map_name]\n",
    "recreation_data.rename(columns=column_rename, inplace=True)\n",
    "\n",
    "# identify the columns of interest\n",
    "rec_groups = recreation_data.OBJEKTART.unique()\n",
    "rec_presence = []\n",
    "\n",
    "groups_and_presence = [*rec_groups, *rec_presence]\n",
    "\n",
    "# update the key word arguments for the land use data\n",
    "land_use_update = dict(data_map=recreation_data, id_vals=id_vals, to_aggregate=agg_val, land_use_groups=rec_groups)\n",
    "land_use_kwargs.update(land_use_update)\n",
    "\n",
    "# update the key word arguments for the test data\n",
    "test_update = dict(groups=rec_groups, presence=None)\n",
    "test_kwargs.update(test_update)\n",
    "\n",
    "kwargs = {\n",
    "    'land_use_kwargs': land_use_kwargs,\n",
    "    'test_kwargs': test_kwargs,\n",
    "    'label': 'recreation',\n",
    "    'assign_undefined': False,\n",
    "    'length': True,\n",
    "    'cover': False,\n",
    "    'inf_operation': inf_operation,\n",
    "    'regional_label': None,\n",
    "    'label_map': None,\n",
    "    'total': True\n",
    "}\n",
    "\n",
    "recreation = LanduseConfiguration(**kwargs)\n",
    "recreation.groups_and_presence()\n",
    "recreation.inference_groups()\n",
    "rec_config = recreation.configuration_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infrastructure\n"
     ]
    }
   ],
   "source": [
    "# add the data from another land use layer\n",
    "# the results will be added to the landcover data frame\n",
    "column_rename = {\"undefined\":\"Undefined\"}\n",
    "data_map_name = 'hex-3000-lake-locations-nutuzungsareal'\n",
    "id_vals = [\"location\", \"OBJEKTART\"]\n",
    "agg_val = \"surface\"\n",
    "\n",
    "infrastructure_data = data_map[data_map_name]\n",
    "infrastructure_data.rename(columns=column_rename, inplace=True)\n",
    "\n",
    "# identify the columns of interest\n",
    "inf_groups = infrastructure_data.OBJEKTART.unique()\n",
    "inf_presence = []\n",
    "\n",
    "groups_and_presence = [*inf_groups, *inf_presence]\n",
    "\n",
    "# update the key word arguments for the land use data\n",
    "land_use_update = dict(data_map=infrastructure_data, id_vals=id_vals, to_aggregate=agg_val, land_use_groups=inf_groups)\n",
    "land_use_kwargs.update(land_use_update)\n",
    "\n",
    "# update the key word arguments for the test data\n",
    "test_update = dict(groups=inf_groups, presence=None)\n",
    "test_kwargs.update(test_update)\n",
    "\n",
    "kwargs = {\n",
    "    'land_use_kwargs': land_use_kwargs,\n",
    "    'test_kwargs': test_kwargs,\n",
    "    'label': 'infrastructure',\n",
    "    'assign_undefined': False,\n",
    "    'length': True,\n",
    "    'cover': False,\n",
    "    'inf_operation': inf_operation,\n",
    "    'regional_label': None,\n",
    "    'label_map': None,\n",
    "    'total': True\n",
    "}\n",
    "\n",
    "infrastructure = LanduseConfiguration(**kwargs)\n",
    "infrastructure.groups_and_presence()\n",
    "infrastructure.inference_groups()\n",
    "inra_config = infrastructure.configuration_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_map_name = 'resources/hex-3000m/lake-locations-distance-to_intersection.csv'\n",
    "intersections = pd.read_csv(data_map_name)\n",
    "intersections = intersections[intersections.location.isin(locations)].copy()\n",
    "intersections = intersections.drop_duplicates([\"location\", \"distance\"])\n",
    "dints = intersections.groupby(\"location\").distance.sum()\n",
    "nints = intersections.groupby(\"location\").distance.nunique()\n",
    "ints_d = pd.DataFrame(dints/nints).reset_index()\n",
    "river_length = pd.read_csv('resources/buffer_output/intersection_length.csv')\n",
    "river_length = river_length[river_length.location.isin(locations)].copy()\n",
    "river_length = river_length.drop_duplicates([\"location\", \"length\"])\n",
    "nlen = river_length.groupby(\"location\").length.nunique()\n",
    "llen = river_length.groupby(\"location\").length.sum()\n",
    "lengths_d = pd.DataFrame(llen/nlen).reset_index()\n",
    "# dist_len = lengths_d.merge(ints_d, on=\"location\").reset_index()\n",
    "# dist_len = pd.melt(dist_len, id_vars=\"location\", value_vars=[\"length\", \"distance\"])\n",
    "# dist_len\n",
    "len_d = pd.melt(lengths_d, id_vars=\"location\", value_vars=\"length\").fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r-length\n"
     ]
    }
   ],
   "source": [
    "\n",
    "id_vals = [\"location\", \"variable\"]\n",
    "agg_val = \"value\"\n",
    "inf_groups = [\"length\"]\n",
    "inf_presence = []\n",
    "\n",
    "groups_and_presence = [*inf_groups, *inf_presence]\n",
    "\n",
    "\n",
    "\n",
    "# update the key word arguments for the land use data\n",
    "land_use_update = dict(data_map=len_d, id_vals=id_vals, to_aggregate=agg_val, land_use_groups=inf_groups)\n",
    "land_use_kwargs.update(land_use_update)\n",
    "\n",
    "# update the key word arguments for the test data\n",
    "test_update = dict(groups=inf_groups, presence=None)\n",
    "test_kwargs.update(test_update)\n",
    "\n",
    "\n",
    "\n",
    "kwargs = {\n",
    "    'land_use_kwargs': land_use_kwargs,\n",
    "    'test_kwargs': test_kwargs,\n",
    "    'label': \"r-length\",\n",
    "    'assign_undefined': False,\n",
    "    'length': True,\n",
    "    'cover': False,\n",
    "    'inf_operation': inf_operation,\n",
    "    'regional_label': None,\n",
    "    'label_map': label_map,\n",
    "    'total': True\n",
    "}\n",
    "\n",
    "intersections = LanduseConfiguration(**kwargs)\n",
    "intersections.groups_and_presence()\n",
    "intersections.inference_groups()\n",
    "int_conf = intersections.configuration_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add results for land use to land cover \n",
    "# drop the conf column\n",
    "fgl = nx_conf.drop(\"conf\", axis=1)\n",
    "\n",
    "fgl['infrastructure'] = fgl.location.apply(lambda x: inra_config.loc[x][0][0])\n",
    "fgl['strasse'] = fgl.location.apply(lambda x: strasse_config.loc[x][0][0])\n",
    "fgl['recreation'] = fgl.location.apply(lambda x: rec_config.loc[x][0][0])\n",
    "fgl['r-length'] = fgl.location.apply(lambda x: int_conf.loc[x][0][0])\n",
    "fgl = fgl.fillna(0)\n",
    "# define the confiugration columns\n",
    "conf_columns = [*lc_groups, *lc_presence, 'infrastructure', 'strasse', 'recreation', 'r-length']\n",
    "fgl['configuration'] = list(zip(*[fgl[x] for x in conf_columns]))\n",
    "fgl_conf_keys = fgl[[\"location\", \"configuration\"]].set_index(\"location\")\n",
    "\n",
    "# update the probability tables\n",
    "fgl_ptables =nx.p_tables\n",
    "fgl_ptables.update(infrastructure.p_tables)\n",
    "fgl_ptables.update(recreation.p_tables)\n",
    "fgl_ptables.update(strasse.p_tables)\n",
    "fgl_ptables.update(intersections.p_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gfoam 0.07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'anarchy-beach': 0.655279488452582,\n",
       " 'baby-plage-geneva': 0.2905146915249824,\n",
       " 'baby-plage-ii-geneve': 0.23486529791743418,\n",
       " 'bain-des-dames': 0.6501828241621083,\n",
       " 'baye-de-montreux-g': 0.7478335812501131,\n",
       " 'boiron': 0.6564500234067034,\n",
       " 'cully-plage': 0.7343319144035644,\n",
       " 'grand-clos': 0.9548329343708758,\n",
       " 'la-pecherie': 0.5942529321416143,\n",
       " 'lacleman_gland_lecoanets': 0.23895209557658775,\n",
       " 'le-pierrier': 0.6459062121316048,\n",
       " 'maladaire': 0.705474136911692,\n",
       " 'oyonne': 0.6499399490718982,\n",
       " 'parc-des-pierrettes': 0.642468747226554,\n",
       " 'plage-de-st-sulpice': 0.3114724772227811,\n",
       " 'preverenges': 0.6283591860128622,\n",
       " 'preverenges-le-sout': 0.6545251586166427,\n",
       " 'quai-maria-belgia': 0.5668653213328498,\n",
       " 'rocky-plage': 0.3131905935493407,\n",
       " 'tiger-duck-beach': 0.7776161323771168,\n",
       " 'tolochenaz': 0.6670945098345765,\n",
       " 'versoix': 0.18715028420355406,\n",
       " 'vidy-ruines': 0.5312928333258125,\n",
       " 'villa-barton': 0.47332959011791126}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_kwargs = {\n",
    "    'location': \" \",\n",
    "    'lake': \"\",\n",
    "    'conf_label': 'configuration',\n",
    "    'conf_columns': conf_columns,\n",
    "    'p_tables': fgl_ptables,\n",
    "    'tested':nx.grouped_data,\n",
    "    'prior':None,\n",
    "    'drop_vals': None\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "bsee_l = nx.grouped_data[nx.grouped_data.lake == \"lac-leman\"].location.unique()\n",
    "res = {}\n",
    "for alocation in bsee_l:\n",
    "    inference_kwargs.update({\"location\":alocation})\n",
    "    p=inference_for_one_location(**inference_kwargs)\n",
    "    res.update({alocation:p})\n",
    "    \n",
    "print(code, threshhold)\n",
    "res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lct = nx.test_results.test_and_merge()\n",
    "sct = strasse.test_results.test_and_merge(cover=False)\n",
    "rct = recreation.test_results.test_and_merge(cover=False)\n",
    "ict = infrastructure.test_results.test_and_merge(cover=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "This script updated 01/05/2023 in Biel, CH\n",
       "\n",
       "> ❤️ what you do everyday\n",
       "\n",
       "*analyst at hammerdirt*\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today = dt.datetime.now().date().strftime(\"%d/%m/%Y\")\n",
    "where = \"Biel, CH\"\n",
    "\n",
    "my_block = f\"\"\"\n",
    "\n",
    "This script updated {today} in {where}\n",
    "\n",
    "> \\u2764\\ufe0f what you do everyday\n",
    "\n",
    "*analyst at hammerdirt*\n",
    "\"\"\"\n",
    "\n",
    "md(my_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Git repo: https://github.com/hammerdirt-analyst/landuse.git\n",
      "\n",
      "Git branch: probability\n",
      "\n",
      "scipy     : 1.10.1\n",
      "PIL       : 9.5.0\n",
      "pandas    : 2.0.0\n",
      "matplotlib: 3.7.1\n",
      "numpy     : 1.24.2\n",
      "json      : 2.0.9\n",
      "IPython   : 8.12.0\n",
      "seaborn   : 0.12.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark --iversions -b -r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
